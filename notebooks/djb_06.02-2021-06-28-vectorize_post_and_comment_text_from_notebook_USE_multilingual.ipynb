{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15151647",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb1b5c",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34ba372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47836cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.4\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    "    D_MODELS_CPU,\n",
    "    process_text_for_fse,\n",
    "    vectorize_text_with_fse,\n",
    ")\n",
    "from subclu.models.preprocess_text import TextPreprocessor, transform_and_tokenize_text\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4858a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328a1e9",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73522a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3272b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/mlflow/mlruns.db'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bac0cf",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577cd166",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlf.list_experiment_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0947c90",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e84f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 0\n",
      "GPU details:\n",
      "[]\n",
      "\n",
      "All devices:\n",
      "===\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16250078998344776964\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12248579071249388595\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "    f\"\\n\\nAll devices:\\n===\\n\"\n",
    "    f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e451ca",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ef47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment = 'use_multilingual_v0.1_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92729112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:19 | INFO | \"Start vectorize function\"\n",
      "18:46:19 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual_large/2021-06-30_1846\"\n",
      "18:46:19 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/de/2021-06-16\"\n",
      "18:46:43 | INFO | \"  0:00:23.870929 <- df_post time elapsed\"\n",
      "18:46:43 | INFO | \"  (262226, 6) <- df_posts.shape\"\n",
      "18:46:43 | INFO | \"Load comments df...\"\n",
      "18:47:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "18:47:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "18:47:08 | INFO | \"  (1108757, 6) <- updated df_comments shape\"\n",
      "18:47:08 | INFO | \"Load subreddits df...\"\n",
      "18:47:10 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "18:47:10 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "18:47:10 | INFO | \"Loading model use_multilingual_large...\n",
      "  with kwargs: None\"\n",
      "18:47:23 | INFO | \"  0:00:13.100165 <- Load TF HUB model time elapsed\"\n",
      "18:47:23 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:47:23 | INFO | \"Getting embeddings in batches of size: 3000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fba36b42c24149a0574945007eaf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_posts, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual_large',\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path='posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=3000,\n",
    "    tf_limit_first_n_chars=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0622c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec787a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2069cc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LEGACY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c5569a5ca02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLEGACY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LEGACY' is not defined"
     ]
    }
   ],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02da8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:25:16 | INFO | \"Start vectorize function\"\n",
      "07:25:16 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0725\"\n",
      "07:25:16 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:25:22 | INFO | \"  0:00:05.708467 <- df_post time elapsed\"\n",
      "07:25:22 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:25:22 | INFO | \"Load comments df...\"\n",
      "07:25:29 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:25:29 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:25:30 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:25:30 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:25:30 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:25:30 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:25:30 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:25:30 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:25:30 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:25:30 | INFO | \"Converting df_train to fse format...\"\n",
      "07:25:30 | INFO | \"  0:00:00.067377 <- Converting to fse time elapsed\"\n",
      "07:25:30 | INFO | \"Logging training df to mlflow...\"\n",
      "07:25:31 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:25:31 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:25:31 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:27:02 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:27:02 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:27:04 | INFO | \"  0:01:33.903202 <- Load FSE model time elapsed\"\n",
      "07:27:04 | INFO | \"Start training fse model...\"\n",
      "07:27:13 | INFO | \"Running inference on all POSTS...\"\n",
      "07:27:13 | INFO | \"Convert vectors to df...\"\n",
      "07:27:19 | INFO | \"(111669, 300) <- Raw vectorized text shape\"\n",
      "07:27:19 | INFO | \"  0:00:05.800530 <- Raw vectorize to df only time elapsed\"\n",
      "07:27:19 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:27:19 | INFO | \"  Setting post_id as index...\"\n",
      "07:27:19 | INFO | \"Merge vectors with df...\"\n",
      "07:27:19 | INFO | \"  0:00:00.344488 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:27:19 | INFO | \"  0:00:06.567358 <- Converting vectors to df full time elapsed\"\n",
      "07:27:19 | INFO | \"Saving inference for comments df\"\n",
      "07:27:23 | INFO | \"  Saving inference complete\"\n",
      "07:27:23 | INFO | \"Get vectors for comments\"\n",
      "07:27:23 | INFO | \"Convert vectors to df...\"\n",
      "07:28:00 | INFO | \"(638052, 300) <- Raw vectorized text shape\"\n",
      "07:28:00 | INFO | \"  0:00:36.960138 <- Raw vectorize to df only time elapsed\"\n",
      "07:28:00 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:28:00 | INFO | \"  Setting comment_id as index...\"\n",
      "07:28:03 | INFO | \"Merge vectors with df...\"\n",
      "07:28:05 | INFO | \"  0:00:02.301038 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.310134 <- Converting vectors to df full time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.806270 <- Inference time for COMMENTS time elapsed\"\n",
      "07:28:05 | INFO | \"Save vectors for comments\"\n",
      "07:28:22 | INFO | \"  0:03:05.402697 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "del model, df_posts, d_ix_to_id\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3476270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:45:47 | INFO | \"Start vectorize function\"\n",
      "07:45:47 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0745\"\n",
      "07:45:47 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:45:52 | INFO | \"  0:00:04.924321 <- df_post time elapsed\"\n",
      "07:45:52 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:45:52 | INFO | \"Load comments df...\"\n",
      "07:45:59 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:45:59 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:45:59 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:45:59 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:46:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:46:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:46:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:46:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:46:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:46:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:46:00 | INFO | \"  0:00:00.080495 <- Converting to fse time elapsed\"\n",
      "07:46:00 | INFO | \"Logging training df to mlflow...\"\n",
      "07:46:00 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:46:00 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:46:00 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:47:37 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:47:37 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:47:40 | INFO | \"  0:01:40.317414 <- Load FSE model time elapsed\"\n",
      "07:47:40 | INFO | \"Start training fse model...\"\n",
      "07:47:52 | INFO | \"Running inference on all POSTS...\"\n",
      "07:47:52 | INFO | \"  Inference + convert to df...\"\n",
      "07:48:02 | INFO | \"  0:00:09.456951 <- Raw inference+df only time elapsed\"\n",
      "07:48:02 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:48:02 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:48:02 | INFO | \"  Setting post_id as index...\"\n",
      "07:48:02 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:48:03 | INFO | \"  0:00:00.478258 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:48:03 | INFO | \"  0:00:10.478047 <- Converting vectors to df FULL time elapsed\"\n",
      "07:48:03 | INFO | \"Saving inference for comments df\"\n",
      "07:48:07 | INFO | \"  Saving inference complete\"\n",
      "07:48:07 | INFO | \"Get vectors for comments\"\n",
      "07:48:07 | INFO | \"  Inference + convert to df...\"\n",
      "07:49:11 | INFO | \"  0:01:03.325285 <- Raw inference+df only time elapsed\"\n",
      "07:49:11 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:49:11 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:49:11 | INFO | \"  Setting comment_id as index...\"\n",
      "07:49:16 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:49:19 | INFO | \"  0:00:02.969392 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:11.900149 <- Converting vectors to df FULL time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:12.535015 <- Inference time for COMMENTS time elapsed\"\n",
      "07:49:19 | INFO | \"Save vectors for comments\"\n",
      "07:49:42 | INFO | \"  0:03:54.915527 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e613a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:49:43 | INFO | \"Start vectorize function\"\n",
      "07:49:43 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0749\"\n",
      "07:49:43 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:49:50 | INFO | \"  0:00:06.091006 <- df_post time elapsed\"\n",
      "07:49:50 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:49:50 | INFO | \"Load comments df...\"\n",
      "07:49:58 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:49:58 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:49:58 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:49:58 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:50:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:50:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:50:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:50:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:50:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:50:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:50:01 | INFO | \"  0:00:00.116683 <- Converting to fse time elapsed\"\n",
      "07:50:01 | INFO | \"Logging training df to mlflow...\"\n",
      "07:50:01 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:50:01 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:50:01 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:51:35 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:51:35 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:51:38 | INFO | \"  0:01:37.257050 <- Load FSE model time elapsed\"\n",
      "07:51:38 | INFO | \"Start training fse model...\"\n",
      "07:51:47 | INFO | \"Running inference on all POSTS...\"\n",
      "07:51:47 | INFO | \"  Inference + convert to df...\"\n",
      "07:51:55 | INFO | \"  0:00:07.726996 <- Raw inference+df only time elapsed\"\n",
      "07:51:55 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:51:55 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:51:55 | INFO | \"  Setting post_id as index...\"\n",
      "07:51:56 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:51:56 | INFO | \"  0:00:00.322830 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:51:56 | INFO | \"  0:00:08.454281 <- Converting vectors to df FULL time elapsed\"\n",
      "07:51:56 | INFO | \"Saving inference for comments df\"\n",
      "07:51:59 | INFO | \"  Saving inference complete\"\n",
      "07:51:59 | INFO | \"Get vectors for comments\"\n",
      "07:52:00 | INFO | \"  Inference + convert to df...\"\n",
      "07:52:49 | INFO | \"  0:00:49.124738 <- Raw inference+df only time elapsed\"\n",
      "07:52:49 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:52:49 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:52:49 | INFO | \"  Setting comment_id as index...\"\n",
      "07:52:53 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:52:55 | INFO | \"  0:00:02.418618 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.369872 <- Converting vectors to df FULL time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.833132 <- Inference time for COMMENTS time elapsed\"\n",
      "07:52:55 | INFO | \"Save vectors for comments\"\n",
      "07:53:14 | INFO | \"  0:03:31.067865 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2030df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ea8181",
   "metadata": {},
   "source": [
    "# Recover artifact from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01638186",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9da4e979eae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'aac3e007dfc2446790e25887adf287f6'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "run_id = 'aac3e007dfc2446790e25887adf287f6'\n",
    "run = mlflow.get_run(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbb99cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b18c9fef2cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34mf\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0ec254",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-532d324e2d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_idx = pd.read_csv(f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\")\n",
    "print(df_idx.shape)\n",
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126f8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
