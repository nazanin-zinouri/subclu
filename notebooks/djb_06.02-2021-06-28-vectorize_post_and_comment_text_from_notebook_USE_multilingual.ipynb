{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd4c66d",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0e1fb",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f50d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18488d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.4\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    "    D_MODELS_CPU,\n",
    "    process_text_for_fse,\n",
    "    vectorize_text_with_fse,\n",
    ")\n",
    "from subclu.models.preprocess_text import TextPreprocessor, transform_and_tokenize_text\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26e58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59e0a8",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46bea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba802ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/mlflow/mlruns.db'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c152dbc",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a606be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7742d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_id': '0',\n",
       "  'name': 'Default',\n",
       "  'artifact_location': './mlruns/0',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '1',\n",
       "  'name': 'use_multilingual_v0.1_test',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/1',\n",
       "  'lifecycle_stage': 'active'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc77d35",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0293c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 0\n",
      "GPU details:\n",
      "[]\n",
      "\n",
      "All devices:\n",
      "===\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17554342817575500758\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5847791926257555059\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "    f\"\\n\\nAll devices:\\n===\\n\"\n",
    "    f\"{device_lib.list_local_devices()}\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9d564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12684331277337632839\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14335225933054774797\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc3d99",
   "metadata": {},
   "source": [
    "# Call function to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3840df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment = 'use_multilingual_v0.1_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c27bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:57:40 | INFO | \"Start vectorize function\"\n",
      "15:57:40 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual_large/2021-06-29_1557\"\n",
      "15:57:40 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/de/2021-06-16\"\n",
      "15:57:50 | INFO | \"  0:00:09.939972 <- df_post time elapsed\"\n",
      "15:57:50 | INFO | \"  (262226, 6) <- df_posts.shape\"\n",
      "15:57:50 | INFO | \"Load comments df...\"\n",
      "15:58:02 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "15:58:03 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "15:58:03 | INFO | \"  (1108757, 6) <- updated df_comments shape\"\n",
      "15:58:03 | INFO | \"Load subreddits df...\"\n",
      "15:58:03 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "15:58:03 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "15:58:04 | INFO | \"Loading model use_multilingual_large...\n",
      "  with kwargs: None\"\n",
      "15:58:04 | INFO | \"Using /tmp/tfhub_modules to cache modules.\"\n",
      "15:58:11 | INFO | \"  0:00:06.638491 <- Load TF HUB model time elapsed\"\n",
      "15:58:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10278314,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderTransformer/Transformer/PrepareForTransformer/dense/BiasAdd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_restored_function_body_51014]\n\nFunction call stack:\nrestored_function_body\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3ae7a2f86c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msubreddits_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'subreddits/de/2021-06-16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mposts_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'posts/de/2021-06-16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcomments_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'comments/de/2021-06-16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text.py\u001b[0m in \u001b[0;36mvectorize_text_to_embeddings\u001b[0;34m(model_name, tokenize_function, tokenize_lowercase, bucket_name, subreddits_path, posts_path, comments_path, preprocess_text_folder, col_text_post, col_text_post_word_count, col_text_post_url, col_post_id, col_comment_id, col_text_comment, col_text_comment_word_count, col_subreddit_id, col_text_subreddit_description, col_text_subreddit_word_count, model_kwargs, train_subreddits_to_exclude, train_exclude_duplicated_docs, train_min_word_count, mlflow_experiment, train_use_comments)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposts_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             df_vect = pd.DataFrame(\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_posts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_posts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_post_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m               *args, **kwds)\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[10278314,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderTransformer/Transformer/PrepareForTransformer/dense/BiasAdd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_restored_function_body_51014]\n\nFunction call stack:\nrestored_function_body\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_posts, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual_large',\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path='posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764ecd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33d79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a20491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:25:16 | INFO | \"Start vectorize function\"\n",
      "07:25:16 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0725\"\n",
      "07:25:16 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:25:22 | INFO | \"  0:00:05.708467 <- df_post time elapsed\"\n",
      "07:25:22 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:25:22 | INFO | \"Load comments df...\"\n",
      "07:25:29 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:25:29 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:25:30 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:25:30 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:25:30 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:25:30 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:25:30 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:25:30 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:25:30 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:25:30 | INFO | \"Converting df_train to fse format...\"\n",
      "07:25:30 | INFO | \"  0:00:00.067377 <- Converting to fse time elapsed\"\n",
      "07:25:30 | INFO | \"Logging training df to mlflow...\"\n",
      "07:25:31 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:25:31 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:25:31 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:27:02 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:27:02 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:27:04 | INFO | \"  0:01:33.903202 <- Load FSE model time elapsed\"\n",
      "07:27:04 | INFO | \"Start training fse model...\"\n",
      "07:27:13 | INFO | \"Running inference on all POSTS...\"\n",
      "07:27:13 | INFO | \"Convert vectors to df...\"\n",
      "07:27:19 | INFO | \"(111669, 300) <- Raw vectorized text shape\"\n",
      "07:27:19 | INFO | \"  0:00:05.800530 <- Raw vectorize to df only time elapsed\"\n",
      "07:27:19 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:27:19 | INFO | \"  Setting post_id as index...\"\n",
      "07:27:19 | INFO | \"Merge vectors with df...\"\n",
      "07:27:19 | INFO | \"  0:00:00.344488 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:27:19 | INFO | \"  0:00:06.567358 <- Converting vectors to df full time elapsed\"\n",
      "07:27:19 | INFO | \"Saving inference for comments df\"\n",
      "07:27:23 | INFO | \"  Saving inference complete\"\n",
      "07:27:23 | INFO | \"Get vectors for comments\"\n",
      "07:27:23 | INFO | \"Convert vectors to df...\"\n",
      "07:28:00 | INFO | \"(638052, 300) <- Raw vectorized text shape\"\n",
      "07:28:00 | INFO | \"  0:00:36.960138 <- Raw vectorize to df only time elapsed\"\n",
      "07:28:00 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:28:00 | INFO | \"  Setting comment_id as index...\"\n",
      "07:28:03 | INFO | \"Merge vectors with df...\"\n",
      "07:28:05 | INFO | \"  0:00:02.301038 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.310134 <- Converting vectors to df full time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.806270 <- Inference time for COMMENTS time elapsed\"\n",
      "07:28:05 | INFO | \"Save vectors for comments\"\n",
      "07:28:22 | INFO | \"  0:03:05.402697 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "del model, df_posts, d_ix_to_id\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481b24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:45:47 | INFO | \"Start vectorize function\"\n",
      "07:45:47 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0745\"\n",
      "07:45:47 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:45:52 | INFO | \"  0:00:04.924321 <- df_post time elapsed\"\n",
      "07:45:52 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:45:52 | INFO | \"Load comments df...\"\n",
      "07:45:59 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:45:59 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:45:59 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:45:59 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:46:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:46:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:46:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:46:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:46:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:46:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:46:00 | INFO | \"  0:00:00.080495 <- Converting to fse time elapsed\"\n",
      "07:46:00 | INFO | \"Logging training df to mlflow...\"\n",
      "07:46:00 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:46:00 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:46:00 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:47:37 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:47:37 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:47:40 | INFO | \"  0:01:40.317414 <- Load FSE model time elapsed\"\n",
      "07:47:40 | INFO | \"Start training fse model...\"\n",
      "07:47:52 | INFO | \"Running inference on all POSTS...\"\n",
      "07:47:52 | INFO | \"  Inference + convert to df...\"\n",
      "07:48:02 | INFO | \"  0:00:09.456951 <- Raw inference+df only time elapsed\"\n",
      "07:48:02 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:48:02 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:48:02 | INFO | \"  Setting post_id as index...\"\n",
      "07:48:02 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:48:03 | INFO | \"  0:00:00.478258 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:48:03 | INFO | \"  0:00:10.478047 <- Converting vectors to df FULL time elapsed\"\n",
      "07:48:03 | INFO | \"Saving inference for comments df\"\n",
      "07:48:07 | INFO | \"  Saving inference complete\"\n",
      "07:48:07 | INFO | \"Get vectors for comments\"\n",
      "07:48:07 | INFO | \"  Inference + convert to df...\"\n",
      "07:49:11 | INFO | \"  0:01:03.325285 <- Raw inference+df only time elapsed\"\n",
      "07:49:11 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:49:11 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:49:11 | INFO | \"  Setting comment_id as index...\"\n",
      "07:49:16 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:49:19 | INFO | \"  0:00:02.969392 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:11.900149 <- Converting vectors to df FULL time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:12.535015 <- Inference time for COMMENTS time elapsed\"\n",
      "07:49:19 | INFO | \"Save vectors for comments\"\n",
      "07:49:42 | INFO | \"  0:03:54.915527 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf30cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:49:43 | INFO | \"Start vectorize function\"\n",
      "07:49:43 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0749\"\n",
      "07:49:43 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:49:50 | INFO | \"  0:00:06.091006 <- df_post time elapsed\"\n",
      "07:49:50 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:49:50 | INFO | \"Load comments df...\"\n",
      "07:49:58 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:49:58 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:49:58 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:49:58 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:50:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:50:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:50:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:50:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:50:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:50:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:50:01 | INFO | \"  0:00:00.116683 <- Converting to fse time elapsed\"\n",
      "07:50:01 | INFO | \"Logging training df to mlflow...\"\n",
      "07:50:01 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:50:01 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:50:01 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:51:35 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:51:35 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:51:38 | INFO | \"  0:01:37.257050 <- Load FSE model time elapsed\"\n",
      "07:51:38 | INFO | \"Start training fse model...\"\n",
      "07:51:47 | INFO | \"Running inference on all POSTS...\"\n",
      "07:51:47 | INFO | \"  Inference + convert to df...\"\n",
      "07:51:55 | INFO | \"  0:00:07.726996 <- Raw inference+df only time elapsed\"\n",
      "07:51:55 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:51:55 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:51:55 | INFO | \"  Setting post_id as index...\"\n",
      "07:51:56 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:51:56 | INFO | \"  0:00:00.322830 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:51:56 | INFO | \"  0:00:08.454281 <- Converting vectors to df FULL time elapsed\"\n",
      "07:51:56 | INFO | \"Saving inference for comments df\"\n",
      "07:51:59 | INFO | \"  Saving inference complete\"\n",
      "07:51:59 | INFO | \"Get vectors for comments\"\n",
      "07:52:00 | INFO | \"  Inference + convert to df...\"\n",
      "07:52:49 | INFO | \"  0:00:49.124738 <- Raw inference+df only time elapsed\"\n",
      "07:52:49 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:52:49 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:52:49 | INFO | \"  Setting comment_id as index...\"\n",
      "07:52:53 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:52:55 | INFO | \"  0:00:02.418618 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.369872 <- Converting vectors to df FULL time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.833132 <- Inference time for COMMENTS time elapsed\"\n",
      "07:52:55 | INFO | \"Save vectors for comments\"\n",
      "07:53:14 | INFO | \"  0:03:31.067865 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce17aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd09153a",
   "metadata": {},
   "source": [
    "# Recover artifact from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72cb3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'aac3e007dfc2446790e25887adf287f6'\n",
    "run = mlflow.get_run(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba17524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://i18n-subreddit-clustering/mlflow/mlruns/4/aac3e007dfc2446790e25887adf287f6/artifacts/d_ix_to_id/d_ix_to_id.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee550a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111669, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_index</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>t3_mkyj2k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>t3_mkynzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>t3_mkyolv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>t3_mkyp17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>t3_mkyqrz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   training_index    post_id\n",
       "0               0  t3_mkyj2k\n",
       "1               1  t3_mkynzi\n",
       "2               2  t3_mkyolv\n",
       "3               3  t3_mkyp17\n",
       "4               4  t3_mkyqrz"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx = pd.read_csv(f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\")\n",
    "print(df_idx.shape)\n",
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923148f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
