{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85e74cf",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-10-05:\n",
    "I ran into memory errors with 600GB or RAM, so here's a try with 1.4TB... if this doesn't work. Then I don't know what will...\n",
    "\n",
    "---\n",
    "\n",
    "2021-08-10: Finally completed testing with sampling <= 10 files. Now ready to run process on full data!\n",
    "\n",
    "Ended up doing it all in dask + pandas + numpy because of problems installing `cuDF`.\n",
    "\n",
    "---\n",
    "2021-08-02: Now that I'm processing millions of comments and posts, I need to re-write the functions to try to do some work in parallel and reduce the amount of data loaded in RAM.\n",
    "\n",
    "- `Dask` seems like a great option to load data and only compute some of it as needed.\n",
    "- `cuDF` could be a way to speed up some computation using GPUs\n",
    "- `Dask-delayed` could be a way to create a task DAG lazily before computing all the aggregates.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In notebook 09 I combined embeddings from posts & subreddits (`djb_09.00-combine_post_and_comments_and_visualize_for_presentation.ipynb`).\n",
    "\n",
    "In this notebook I'll be testing functions that include mlflow so that it's easier to try a lot of different weights to find better respresentations.\n",
    "\n",
    "Take embeddings created by other models & combine them:\n",
    "```\n",
    "new post embeddings = post + comments + subreddit description\n",
    "\n",
    "new subreddit embeddings = new posts (weighted by post length or upvotes?)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a763f3",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25627793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be46a051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bad09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b95a0e",
   "metadata": {},
   "source": [
    "# Set sqlite database as MLflow URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49727182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c89066",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731ed54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb21bb",
   "metadata": {},
   "source": [
    "## Get runs that we can use for embeddings aggregation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff3ff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.6 ms, sys: 633 Âµs, total: 89.2 ms\n",
      "Wall time: 88.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44, 120)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_mlf_runs =  mlf.search_all_runs(experiment_ids=[13, 14, 15, 16])\n",
    "df_mlf_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c2e603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_finished = df_mlf_runs['status'] == 'FINISHED'\n",
    "mask_output_over_1M_rows = (\n",
    "    (df_mlf_runs['metrics.df_vect_posts_rows'] >= 1e5) |\n",
    "    (df_mlf_runs['metrics.df_vect_comments'] >= 1e5)\n",
    ")\n",
    "# df_mlf_runs[mask_finished].shape\n",
    "\n",
    "df_mlf_use_for_agg = df_mlf_runs[mask_output_over_1M_rows]\n",
    "df_mlf_use_for_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1519d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_6ea60_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >run id</th>        <th class=\"col_heading level0 col1\" >experiment id</th>        <th class=\"col_heading level0 col2\" >start time</th>        <th class=\"col_heading level0 col3\" >metrics.total comment files processed</th>        <th class=\"col_heading level0 col4\" >metrics.df vect comments</th>        <th class=\"col_heading level0 col5\" >metrics.vectorizing time minutes comments</th>        <th class=\"col_heading level0 col6\" >metrics.vectorizing time minutes full function</th>        <th class=\"col_heading level0 col7\" >params.n sample comment files</th>        <th class=\"col_heading level0 col8\" >params.tf batch inference rows</th>        <th class=\"col_heading level0 col9\" >params.n comment files slice start</th>        <th class=\"col_heading level0 col10\" >params.n comment files slice end</th>        <th class=\"col_heading level0 col11\" >tags.mlflow.runName</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_6ea60_level0_row0\" class=\"row_heading level0 row0\" >18</th>\n",
       "                        <td id=\"T_6ea60_row0_col0\" class=\"data row0 col0\" >deb3454ece2a4a8d8e4149c2d8494c0d</td>\n",
       "                        <td id=\"T_6ea60_row0_col1\" class=\"data row0 col1\" >14</td>\n",
       "                        <td id=\"T_6ea60_row0_col2\" class=\"data row0 col2\" >2021-10-05 01:44:32.386000+00:00</td>\n",
       "                        <td id=\"T_6ea60_row0_col3\" class=\"data row0 col3\" >15.00</td>\n",
       "                        <td id=\"T_6ea60_row0_col4\" class=\"data row0 col4\" >10,121,046.00</td>\n",
       "                        <td id=\"T_6ea60_row0_col5\" class=\"data row0 col5\" >39.11</td>\n",
       "                        <td id=\"T_6ea60_row0_col6\" class=\"data row0 col6\" >45.94</td>\n",
       "                        <td id=\"T_6ea60_row0_col7\" class=\"data row0 col7\" >15</td>\n",
       "                        <td id=\"T_6ea60_row0_col8\" class=\"data row0 col8\" >3200</td>\n",
       "                        <td id=\"T_6ea60_row0_col9\" class=\"data row0 col9\" >None</td>\n",
       "                        <td id=\"T_6ea60_row0_col10\" class=\"data row0 col10\" >None</td>\n",
       "                        <td id=\"T_6ea60_row0_col11\" class=\"data row0 col11\" >comments_batch_01-2021-10-05_014431</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6ea60_level0_row1\" class=\"row_heading level0 row1\" >19</th>\n",
       "                        <td id=\"T_6ea60_row1_col0\" class=\"data row1 col0\" >5f10cd75334142168a6ebb787e477c1f</td>\n",
       "                        <td id=\"T_6ea60_row1_col1\" class=\"data row1 col1\" >14</td>\n",
       "                        <td id=\"T_6ea60_row1_col2\" class=\"data row1 col2\" >2021-10-05 00:22:20.334000+00:00</td>\n",
       "                        <td id=\"T_6ea60_row1_col3\" class=\"data row1 col3\" >20.00</td>\n",
       "                        <td id=\"T_6ea60_row1_col4\" class=\"data row1 col4\" >13,558,304.00</td>\n",
       "                        <td id=\"T_6ea60_row1_col5\" class=\"data row1 col5\" >47.64</td>\n",
       "                        <td id=\"T_6ea60_row1_col6\" class=\"data row1 col6\" >57.33</td>\n",
       "                        <td id=\"T_6ea60_row1_col7\" class=\"data row1 col7\" >20</td>\n",
       "                        <td id=\"T_6ea60_row1_col8\" class=\"data row1 col8\" >4200</td>\n",
       "                        <td id=\"T_6ea60_row1_col9\" class=\"data row1 col9\" >None</td>\n",
       "                        <td id=\"T_6ea60_row1_col10\" class=\"data row1 col10\" >None</td>\n",
       "                        <td id=\"T_6ea60_row1_col11\" class=\"data row1 col11\" >comments_batch_01-2021-10-05_002219</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6ea60_level0_row2\" class=\"row_heading level0 row2\" >23</th>\n",
       "                        <td id=\"T_6ea60_row2_col0\" class=\"data row2 col0\" >9a27f9a72cf348c98d50f486abf3b009</td>\n",
       "                        <td id=\"T_6ea60_row2_col1\" class=\"data row2 col1\" >13</td>\n",
       "                        <td id=\"T_6ea60_row2_col2\" class=\"data row2 col2\" >2021-10-04 22:21:46.401000+00:00</td>\n",
       "                        <td id=\"T_6ea60_row2_col3\" class=\"data row2 col3\" >2.00</td>\n",
       "                        <td id=\"T_6ea60_row2_col4\" class=\"data row2 col4\" >1,286,661.00</td>\n",
       "                        <td id=\"T_6ea60_row2_col5\" class=\"data row2 col5\" >3.93</td>\n",
       "                        <td id=\"T_6ea60_row2_col6\" class=\"data row2 col6\" >5.03</td>\n",
       "                        <td id=\"T_6ea60_row2_col7\" class=\"data row2 col7\" >2</td>\n",
       "                        <td id=\"T_6ea60_row2_col8\" class=\"data row2 col8\" >6000</td>\n",
       "                        <td id=\"T_6ea60_row2_col9\" class=\"data row2 col9\" >None</td>\n",
       "                        <td id=\"T_6ea60_row2_col10\" class=\"data row2 col10\" >None</td>\n",
       "                        <td id=\"T_6ea60_row2_col11\" class=\"data row2 col11\" >posts_as_comments_full_text-2021-10-04_222146</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6241ebf090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_multiple_vals = df_mlf_use_for_agg.columns[df_mlf_use_for_agg.nunique(dropna=False) > 1]\n",
    "# len(cols_with_multiple_vals)\n",
    "\n",
    "style_df_numeric(\n",
    "    df_mlf_use_for_agg\n",
    "    [cols_with_multiple_vals]\n",
    "    .drop(['artifact_uri', 'end_time',\n",
    "           # 'start_time',\n",
    "           ], \n",
    "          axis=1)\n",
    "    .dropna(axis='columns', how='all')\n",
    "    .iloc[:, :30]\n",
    "    ,\n",
    "    rename_cols_for_display=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66dcb0",
   "metadata": {},
   "source": [
    "# Load configs for aggregation jobs\n",
    "\n",
    "`n_sample_comments_files` and `n_sample_posts_files` allow us to only load a few files at a time (e.g., 2 instead of 50) to test the process end-to-end.\n",
    "\n",
    "---\n",
    "Note that by default `hydra` is a cli tool. If we want to call use it in jupyter, we need to manually initialize configs & compose the configuration. See my custom function `load_config_agg_jupyter`. Also see:\n",
    "- [Notebook with `Hydra` examples in a notebook](https://github.com/facebookresearch/hydra/blob/master/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb).\n",
    "- [Hydra docs, Hydra in Jupyter](https://hydra.cc/docs/next/advanced/jupyter_notebooks/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2026373",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=4',     # \n",
    "               'n_sample_comments_files=4',  # 6 is limit for logging unique counts at comment level\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "config_full_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "               'n_sample_posts_files=null', \n",
    "               'n_sample_comments_files=null',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "# config_full_lc_true = AggregateEmbeddingsConfig(\n",
    "#     config_path=\"../config\",\n",
    "#     config_name='aggregate_embeddings',\n",
    "#     overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "#                'n_sample_posts_files=null', \n",
    "#                'n_sample_comments_files=null',\n",
    "#                'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_true',\n",
    "#               ]\n",
    "# )\n",
    "# pprint(config_test_sample_lc_false.config_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a58587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_test_sample_lc_false.config_flat,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d44ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_configs = pd.DataFrame(\n",
    "    [\n",
    "        config_test_sample_lc_false.config_flat,\n",
    "        # config_test_full_lc_false.config_flat,\n",
    "        config_full_lc_false.config_flat,\n",
    "        # config_full_lc_true.config_flat,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1b6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>comments_uuid</th>\n",
       "      <th>mlflow_experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comments_vectorized_mlflow_uuids       posts_vectorized_mlflow_uuids posts_vectorized_mlflow_uuids_lowercase subreddit_meta_vectorized_mlflow_uuids subreddit_meta_vectorized_mlflow_uuids_lowercase  \\\n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "\n",
       "                                                          comments_uuid                 mlflow_experiment  \n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  v0.4.0_use_multi_aggregates_test  \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]       v0.4.0_use_multi_aggregates  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can't use (df_configs.nunique(dropna=False) > 1)\n",
    "#  because when a col's content is a list or something unhashable, we get an error\n",
    "#  so instead we'll check each column individually\n",
    "\n",
    "# cols_with_diffs_config = df_configs.columns[df_configs.nunique(dropna=False) > 1]\n",
    "cols_with_diffs_config = list()\n",
    "for c_ in df_configs.columns:\n",
    "    try:\n",
    "        if df_configs[c_].nunique() > 1:\n",
    "            cols_with_diffs_config.append(c_)\n",
    "    except TypeError:\n",
    "        cols_with_diffs_config.append(c_)\n",
    "        \n",
    "\n",
    "df_configs[cols_with_diffs_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af4b90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(config_test_sample_lc_false.config_flat, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dfe191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43324a64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Initialize a local dask client\n",
    "so that we can see the progress/process for dask jobs.\n",
    "\n",
    "2021-10-06: \n",
    "\n",
    "With 96 CPUs & 1.4 TB of RAM\n",
    "- By default, dask starts 12 workers with this config.\n",
    "- the cluster kept getting stuck when I was only using 8 workers.\n",
    "- I changed it to 10 workers and jobs were movig forward until it ran out of memory too many times.\n",
    "\n",
    "Let's try 9 workers as a middle path?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**dask default**: 8 workers with 64 CPUs present<br>\n",
    "I tested: 16 & 12 workers, but they runs out of RAM and job crashes.\n",
    "\n",
    "I rolled back to 8 workers to complete job /prevent OOM errors even if it feels like it's wasting workers, at least it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fd4f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 526 ms, sys: 157 ms, total: 684 ms\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# dask default: 8 workers with 64 CPUs present,\n",
    "cluster = LocalCluster(n_workers=9)  # 8 too few, 10 too many, what does it pick by default?\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67d5eda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:8787/status'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59f9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b81ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:00:54 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "11:00:54 | INFO | \"cpu_count: 96\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.log_param_hostname()\n",
    "mlf.log_cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa96c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:00:55 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.16%', 'memory_total': '1,444,963', 'memory_used': '2,305', 'memory_free': '1,319,944'}\"\n",
      "11:00:55 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.16%', 'memory_used': '2,309'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.0015979647921780696, 'memory_used': 2309}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.log_ram_stats()\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f603085",
   "metadata": {},
   "source": [
    "# Run Full data with `lower_case=False`\n",
    "\n",
    "The logic for sampling files and download/`caching` files locally lives in the `mlf` custom function.\n",
    "\n",
    "Caching can save 9+ minutes if we try to download the files from GCS every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b9ad104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates\n",
      "n_sample_posts_files: \tNone\n",
      "n_sample_comments_files: \tNone\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t2\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tTrue\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_full_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7e52f65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BREAK' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-66db536c6cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBREAK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BREAK' is not defined"
     ]
    }
   ],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7f236c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:01:02 | INFO | \"== Start run_aggregation() method ==\"\n",
      "11:01:02 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "11:01:02 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "11:01:02 | INFO | \"cpu_count: 96\"\n",
      "11:01:02 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.16%', 'memory_total': '1,444,963', 'memory_used': '2,316', 'memory_free': '1,319,933'}\"\n",
      "11:01:03 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-06_110103-agg_full_lc_false-2021-10-06_110102\"\n",
      "11:01:03 | INFO | \"  Saving config to local path...\"\n",
      "11:01:03 | INFO | \"  Logging config to mlflow...\"\n",
      "11:01:03 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "11:01:03 | INFO | \"Loading subreddit description embeddings...\"\n",
      "11:01:04 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_subreddits_description\"\n",
      "100%|##########################################| 5/5 [00:00<00:00, 17652.79it/s]\n",
      "11:01:04 | INFO | \"  Parquet files found: 2\"\n",
      "11:01:05 | INFO | \"      19,262 |  513 <- Raw vectorized subreddit description shape\"\n",
      "11:01:05 | INFO | \"  Unique check for subreddit description...\"\n",
      "11:01:06 | INFO | \"Loading POSTS embeddings...\"\n",
      "11:01:07 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_posts\"\n",
      "100%|########################################| 28/28 [00:00<00:00, 30767.75it/s]\n",
      "11:01:07 | INFO | \"  Parquet files found: 27\"\n",
      "11:01:07 | INFO | \"  Getting df_v_posts.shape ...\"\n",
      "11:01:14 | INFO | \"   8,439,672 |  514 <- Raw POSTS shape\"\n",
      "11:01:14 | INFO | \"  Checking that posts are unique...\"\n",
      "11:01:30 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "11:01:30 | INFO | \"  Found 2 run UUIDs with COMMENT embeddings...\"\n",
      "11:01:31 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/5f10cd75334142168a6ebb787e477c1f/artifacts/df_vect_comments\"\n",
      "100%|########################################| 21/21 [00:00<00:00, 40627.48it/s]\n",
      "11:01:31 | INFO | \"  Parquet files found: 20\"\n",
      "11:01:31 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
      "100%|########################################| 40/40 [00:00<00:00, 23639.87it/s]\n",
      "11:01:32 | INFO | \"  Parquet files found: 39\"\n",
      "11:01:32 | INFO | \"  0:00:28.808500 <- Total raw embeddings load time elapsed\"\n",
      "11:01:32 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '2.56%', 'memory_used': '36,968'}\"\n",
      "11:01:32 | INFO | \"-- Start _load_metadata() method --\"\n",
      "11:01:32 | INFO | \"Loading POSTS metadata...\"\n",
      "11:01:32 | INFO | \"Reading raw data...\"\n",
      "11:01:32 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-09-27\"\n",
      "100%|###############################| 27/27 [00:00<00:00, 8460.68it/s]\n",
      "11:01:42 | INFO | \"  Applying transformations...\"\n",
      "11:02:17 | INFO | \"  (8439672, 15) <- Raw META POSTS shape\"\n",
      "11:02:17 | INFO | \"Loading subs metadata...\"\n",
      "11:02:17 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "11:02:17 | INFO | \"Reading raw data...\"\n",
      "11:02:17 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-09-24\"\n",
      "100%|#################################| 1/1 [00:00<00:00, 4848.91it/s]\n",
      "11:02:18 | INFO | \"  Applying transformations...\"\n",
      "11:02:27 | INFO | \"  (19262, 91) <- Raw META subreddit description shape\"\n",
      "11:02:27 | INFO | \"Loading COMMENTS metadata...\"\n",
      "11:02:27 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
      "100%|##############################| 59/59 [00:00<00:00, 48780.59it/s]\n",
      "11:02:27 | INFO | \"  (Delayed('int-943dbcef-3c91-4fb1-8898-c2cdd12aae5a'), 7) <- Raw META COMMENTS shape\"\n",
      "11:02:27 | INFO | \"  0:00:54.924089 <- Total metadata loading time elapsed\"\n",
      "11:02:27 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '2.70%', 'memory_used': '39,024'}\"\n",
      "11:02:28 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "11:02:28 | INFO | \"Getting count of comments per post...\"\n",
      "11:02:28 | INFO | \"Create MASK of posts with comments...\"\n",
      "11:02:54 | INFO | \"Concat dfs: comment_count>=1 & comment_count==0\"\n",
      "11:02:54 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "11:05:47 | INFO | \"      775,092 <- Comments that DON'T need to be averaged\"\n",
      "11:05:47 | INFO | \"   39,126,876 <- Comments that need to be averaged\"\n",
      "11:05:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "(\"('dataframe-groupby-count-agg-74b774925f5f2a72d7c6318b1e2d4dcd', 0)\", <WorkerState 'tcp://127.0.0.1:43891', name: 6, memory: 0, processing: 1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36mrun_aggregation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# - up-votes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_comments_to_post_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_ram_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_memory_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36m_agg_comments_to_post_level\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m#        - in this loop check compute the values of the unique() and count() fxns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;31m#  Alternative: don't compute in that loop and create a 2nd loop where the tuples actually get compared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mr_com_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_v_com_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_post_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mc_com_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_v_com_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactive_run\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1986\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m             )\n\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             return sync(\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m             )\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: (\"('dataframe-groupby-count-agg-74b774925f5f2a72d7c6318b1e2d4dcd', 0)\", <WorkerState 'tcp://127.0.0.1:43891', name: 6, memory: 0, processing: 1>)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    job_agg1._send_log_file_to_mlflow()\n",
    "    mlflow.end_run(\"FAILED\")\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    \n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "\n",
    "\n",
    "job_agg1 = AggregateEmbeddings(\n",
    "    run_name=f\"agg_full_lc_false-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_full_lc_false.config_flat\n",
    ")\n",
    "job_agg1.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bad07d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:18:04 | INFO | \"Logging log-file to mlflow...\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_agg1._send_log_file_to_mlflow()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b8df8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c4a8ff",
   "metadata": {},
   "source": [
    "# Run full data, `lower_case=True`\n",
    "\n",
    "Looks like the problem I ran into with the file being corrupted might've been a problem with downloading the file(s). Fix: delete the local cache and download the files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02bb5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:51 | INFO | \"== Start run_aggregation() method ==\"\n",
      "15:47:51 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "15:47:52 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-08-10_154752-full_lc_true-2021-08-10_154751\"\n",
      "15:47:52 | INFO | \"  Saving config to local path...\"\n",
      "15:47:52 | INFO | \"  Logging config to mlflow...\"\n",
      "15:47:52 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "15:47:52 | INFO | \"Loading subreddit description embeddings...\"\n",
      "15:47:53 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_subreddits_description\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7e6e4097cc4648a0a6dcf405072654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:54 | INFO | \"  Reading 1 files\"\n",
      "15:47:55 | INFO | \"       3,767 |  513 <- Raw vectorized subreddit description shape\"\n",
      "15:47:56 | INFO | \"Loading POSTS embeddings...\"\n",
      "15:47:57 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_posts\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b70848d70c4886834fc853336b4b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:44 | INFO | \"  Reading 48 files\"\n",
      "15:48:47 | INFO | \"   1,649,929 |  514 <- Raw POSTS shape\"\n",
      "15:48:51 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "15:48:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_comments\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ab07ab988b41b9b0d92315679c18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:48 | INFO | \"  Reading 37 files\"\n",
      "15:54:49 | INFO | \"  0:06:56.293258 <- Total raw embeddings load time elapsed\"\n",
      "15:54:49 | INFO | \"-- Start _load_metadata() method --\"\n",
      "15:54:49 | INFO | \"Loading POSTS metadata...\"\n",
      "15:54:49 | INFO | \"Reading raw data...\"\n",
      "15:54:49 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59eb9efacfd0489f806b804fd167453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:51 | INFO | \"  Applying transformations...\"\n",
      "15:54:52 | INFO | \"  (1649929, 14) <- Raw META POSTS shape\"\n",
      "15:54:52 | INFO | \"Loading subs metadata...\"\n",
      "15:54:52 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "15:54:52 | INFO | \"Reading raw data...\"\n",
      "15:54:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98ebb0befa4f09aa0cc006cb99f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:53 | INFO | \"  Applying transformations...\"\n",
      "15:54:54 | INFO | \"  (3767, 38) <- Raw META subreddit description shape\"\n",
      "15:54:54 | INFO | \"Loading COMMENTS metadata...\"\n",
      "15:54:54 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-07-09\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e52ad01a47422bbde292fb39492af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:55 | INFO | \"  (Delayed('int-11aa2518-d088-4702-bee1-c90e9c40927d'), 7) <- Raw META COMMENTS shape\"\n",
      "15:54:55 | INFO | \"  0:00:05.773888 <- Total metadata loading time elapsed\"\n",
      "15:54:55 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "15:54:55 | INFO | \"Getting count of comments per post...\"\n",
      "15:55:17 | WARNING | \"Error creating summary of comments per post.\n",
      "'<=' not supported between instances of 'NoneType' and 'int'\"\n",
      "15:55:18 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "15:56:48 | INFO | \"      126,642 <- Comments that DON'T need to be averaged\"\n",
      "15:56:48 | INFO | \"   19,041,512 <- Comments that need to be averaged\"\n",
      "15:56:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "15:59:15 | INFO | \"      979,701 |  514 <- df_v_com_agg SHAPE\"\n",
      "15:59:15 | INFO | \"  0:04:20.021986 <- Total comments to post agg loading time elapsed\"\n",
      "15:59:15 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
      "15:59:17 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
      "15:59:17 | INFO | \"  (Delayed('int-2f2dd3eb-5832-498e-ac7f-0f8fc90cbef9'), 513) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
      "15:59:17 | INFO | \"Concat aggregated comments+posts with posts-without comments\"\n",
      "16:09:32 | INFO | \"    1,649,929 |  514 <- df_posts_agg_b shape after aggregation\"\n",
      "16:09:32 | INFO | \"  0:10:16.855088 <- Total (df_posts_agg_b) posts & comments agg time elapsed\"\n",
      "16:09:32 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-2afffaf2-f55e-4a71-b7a9-43db8c7e9419'), 513) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "16:09:35 | INFO | \"  0:00:02.606942 <- Total (df_posts_agg_c) posts+comments+subs agg time elapsed\"\n",
      "16:09:35 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "16:09:35 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "16:09:35 | INFO | \"A - posts only\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-c0cdc12a-7599-4f26-8877-628f56e69ca7'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
      "16:09:35 | INFO | \"B - posts + comments\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-4a88fcb6-f299-4f12-96d0-1c52f2200024'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
      "16:09:35 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "16:09:36 | INFO | \"  (Delayed('int-2c3b142e-c4c6-44e7-8f3b-5b7bafa6f4a0'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "16:09:36 | INFO | \"  0:00:01.266842 <- Total for ALL subreddit-level agg time elapsed\"\n",
      "16:09:36 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
      "16:09:36 | INFO | \"A...\"\n",
      "16:09:48 | INFO | \"  (3767, 3767) <- df_subs_agg_a_similarity.shape\"\n",
      "16:10:07 | INFO | \"Merge distance + metadata...\"\n",
      "16:10:37 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:10:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:10:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:10:50 | INFO | \"B...\"\n",
      "16:29:57 | INFO | \"  (3767, 3767) <- df_subs_agg_b_similarity.shape\"\n",
      "16:30:16 | INFO | \"Merge distance + metadata...\"\n",
      "16:30:46 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:30:54 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:30:54 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:31:00 | INFO | \"C...\"\n",
      "17:17:45 | INFO | \"  (3767, 3767) <- df_subs_agg_c_similarity.shape\"\n",
      "17:18:06 | INFO | \"Merge distance + metadata...\"\n",
      "17:18:36 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "17:18:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "17:18:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "17:18:49 | INFO | \"  1:09:13.049794 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
      "17:18:49 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "17:18:49 | INFO | \"  Saving config to local path...\"\n",
      "17:18:49 | INFO | \"  Logging config to mlflow...\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f472b6e6ac4d91b6d55cd455fbfc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:18:50 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "17:18:50 | INFO | \"Saving locally...\"\n",
      "17:42:53 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:06:23 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:25 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
      "18:06:25 | INFO | \"Saving locally...\"\n",
      "18:06:25 | INFO | \"Keeping index intact...\"\n",
      "18:06:25 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:25 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:06:25 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:06:29 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:31 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
      "18:06:31 | INFO | \"Saving locally...\"\n",
      "18:06:33 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:40 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:06:40 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "18:06:53 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:07:16 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
      "18:07:16 | INFO | \"Saving locally...\"\n",
      "18:17:47 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:28:42 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:44 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity **\"\n",
      "18:28:44 | INFO | \"Saving locally...\"\n",
      "18:28:44 | INFO | \"Keeping index intact...\"\n",
      "18:28:44 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:44 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:28:44 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:28:48 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:50 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity_pair **\"\n",
      "18:28:50 | INFO | \"Saving locally...\"\n",
      "18:28:52 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:59 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:28:59 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# mlflow.end_run(\"FAILED\")\n",
    "# gc.collect()\n",
    "# try:\n",
    "#     # run setup_logging() to remove logging to the file of a failed job\n",
    "#     setup_logging()\n",
    "    \n",
    "#     del job_agg2\n",
    "#     del d_dfs2\n",
    "# except NameError:\n",
    "#     pass\n",
    "# gc.collect()\n",
    "\n",
    "# job_agg2 = AggregateEmbeddings(\n",
    "#     run_name=f\"full_lc_true-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     **config_full_lc_true.config_flat\n",
    "# )\n",
    "# job_agg2.run_aggregation()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e76fd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33e43c",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de143ec",
   "metadata": {},
   "source": [
    "# Run test with `lower_case=False\n",
    "\n",
    "Sample only a few files in comments/ posts to make sure that job completes even when we're testing new code/logic.\n",
    "\n",
    "Limit to only 2 files of each kind to get minimum test to run end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eeddb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run setup_logging() to remove logging to the file of a failed job\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9cd961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:02:56 | INFO | \"info test\"\n",
      "08:02:56 | WARNING | \"warning message\"\n",
      "08:02:56 | ERROR | \"error message\"\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"debug test\")\n",
    "logging.info(\"info test\")\n",
    "logging.warning(\"warning message\")\n",
    "logging.error(\"error message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e20d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=2',     # \n",
    "               'n_sample_comments_files=2',  # 6 is limit for logging unique counts at comment level\n",
    "               'calculate_similarites=false',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f18cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
      "n_sample_posts_files: \t2\n",
      "n_sample_comments_files: \t2\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t2\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tFalse\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_test_sample_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84abe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27c434af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:03:20 | INFO | \"== Start run_aggregation() method ==\"\n",
      "08:03:20 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "08:03:20 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "08:03:20 | INFO | \"cpu_count: 96\"\n",
      "08:03:20 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.15%', 'memory_total': '1,444,963', 'memory_used': '2,221', 'memory_free': '1,441,429'}\"\n",
      "08:03:20 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-06_080320-sample_test_lc_false-2021-10-06_080319\"\n",
      "08:03:20 | INFO | \"  Saving config to local path...\"\n",
      "08:03:20 | INFO | \"  Logging config to mlflow...\"\n",
      "08:03:21 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "08:03:21 | INFO | \"Loading subreddit description embeddings...\"\n",
      "08:03:22 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_subreddits_description\"\n",
      "100%|##########################################| 5/5 [00:00<00:00, 23616.58it/s]\n",
      "08:03:22 | INFO | \"  Parquet files found: 2\"\n",
      "08:03:23 | INFO | \"      19,262 |  513 <- Raw vectorized subreddit description shape\"\n",
      "08:03:23 | INFO | \"  Unique check for subreddit description...\"\n",
      "08:03:24 | INFO | \"Loading POSTS embeddings...\"\n",
      "08:03:24 | INFO | \"  Sampling POSTS FILES down to: 2\"\n",
      "08:03:25 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_posts\"\n",
      "100%|########################################| 28/28 [00:00<00:00, 32087.57it/s]\n",
      "08:03:25 | INFO | \"  Parquet files found: 2\"\n",
      "08:03:47 | INFO | \"     702,743 |  514 <- Raw POSTS shape\"\n",
      "08:03:47 | INFO | \"  Checking that posts are unique...\"\n",
      "08:03:51 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "08:03:51 | INFO | \"  Sampling COMMENTS FILES down to: 2\"\n",
      "08:03:51 | INFO | \"  Found 2 run UUIDs with COMMENT embeddings...\"\n",
      "08:03:51 | INFO | \"    Sampling 1 FILES per run UUID\"\n",
      "08:03:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/5f10cd75334142168a6ebb787e477c1f/artifacts/df_vect_comments\"\n",
      "100%|########################################| 21/21 [00:00<00:00, 48026.38it/s]\n",
      "08:03:52 | INFO | \"  Parquet files found: 1\"\n",
      "08:03:53 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
      "100%|########################################| 40/40 [00:00<00:00, 46655.22it/s]\n",
      "08:03:53 | INFO | \"  Parquet files found: 1\"\n",
      "08:03:54 | INFO | \"  Keep only comments for posts with embeddings\"\n",
      "08:04:34 | INFO | \"      121,345 |  515 <- COMMENTS shape, after keeping only existing posts\"\n",
      "08:04:34 | INFO | \"  0:01:13.294927 <- Total raw embeddings load time elapsed\"\n",
      "08:04:34 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.60%', 'memory_used': '8,693'}\"\n",
      "08:04:35 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.006016070999741862'.\"\n",
      "08:04:35 | INFO | \"-- Start _load_metadata() method --\"\n",
      "08:04:35 | INFO | \"Loading POSTS metadata...\"\n",
      "08:04:35 | INFO | \"Reading raw data...\"\n",
      "08:04:35 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-09-27\"\n",
      "100%|##############################| 27/27 [00:00<00:00, 40229.56it/s]\n",
      "08:04:46 | INFO | \"  Applying transformations...\"\n",
      "08:05:22 | INFO | \"  (8439672, 15) <- Raw META POSTS shape\"\n",
      "08:05:22 | INFO | \"Loading subs metadata...\"\n",
      "08:05:22 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "08:05:22 | INFO | \"Reading raw data...\"\n",
      "08:05:22 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-09-24\"\n",
      "100%|#################################| 1/1 [00:00<00:00, 6775.94it/s]\n",
      "08:05:23 | INFO | \"  Applying transformations...\"\n",
      "08:05:32 | INFO | \"  (19262, 91) <- Raw META subreddit description shape\"\n",
      "08:05:32 | INFO | \"Loading COMMENTS metadata...\"\n",
      "08:05:32 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
      "100%|##############################| 59/59 [00:00<00:00, 58446.84it/s]\n",
      "08:05:32 | INFO | \"  (Delayed('int-d34daecc-c8e6-4af2-925e-bee0ff39c002'), 7) <- Raw META COMMENTS shape\"\n",
      "08:05:32 | INFO | \"  0:00:57.895196 <- Total metadata loading time elapsed\"\n",
      "08:05:33 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '0.74%', 'memory_used': '10,663'}\"\n",
      "08:05:33 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.007379427708529561'.\"\n",
      "08:05:33 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "08:05:33 | INFO | \"Getting count of comments per post...\"\n",
      "08:05:49 | INFO | \"Comments per post summary:\n",
      "    comment_count  posts_count  percent_of_posts\n",
      "0               0       680254          0.967998\n",
      "1               1         3082          0.004386\n",
      "2               2         2978          0.004238\n",
      "3               3         2492          0.003546\n",
      "4               4         2011          0.002862\n",
      "5               5         1619          0.002304\n",
      "6               6         1295          0.001843\n",
      "7               7         1051          0.001496\n",
      "8               8          838          0.001192\n",
      "9               9         6439          0.009163\n",
      "10             10          242          0.000344\n",
      "11             11          157          0.000223\n",
      "12             12          100          0.000142\n",
      "13             13           68          0.000097\n",
      "14             14           45          0.000064\"\n",
      "08:05:49 | INFO | \"TESTING that all post IDs are counted in df_comment_count_per_post...\"\n",
      "08:06:01 | INFO | \"  True <- Post IDs in df_v_posts == df_comment_count_per_post\"\n",
      "08:06:02 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "08:06:18 | INFO | \"        3,082 <- Comments that DON'T need to be averaged\"\n",
      "08:06:18 | INFO | \"      118,263 <- Comments that need to be averaged\"\n",
      "08:06:18 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "08:06:29 | INFO | \"       22,489 |  514 <- df_v_com_agg SHAPE\"\n",
      "08:06:29 | INFO | \"  0:00:56.043953 <- Total comments to post agg loading time elapsed\"\n",
      "08:06:29 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '2.53%', 'memory_used': '36,507'}\"\n",
      "08:06:29 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.02526500678564088'.\"\n",
      "08:06:29 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
      "08:06:34 | INFO | \"      680,254 <- Posts that DON'T need weighted average\"\n",
      "08:06:34 | INFO | \"       22,489 <- Posts that need weighted average\"\n",
      "08:06:44 | INFO | \"       44,978 |  515 <- Shape of df(posts+comments) to weight\"\n",
      "08:06:44 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
      "08:06:45 | INFO | \"  (Delayed('int-c8339bda-bcc2-4487-969e-2d96f388600a'), 513) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
      "08:06:45 | INFO | \"Concat aggregated comments+posts with posts-without comments\"\n",
      "08:07:12 | INFO | \"      702,743 |  514 <- df_posts_agg_b shape after aggregation\"\n",
      "08:07:12 | INFO | \"  0:00:43.297302 <- Total (df_posts_agg_b) posts & comments agg time elapsed\"\n",
      "08:07:13 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.20%', 'memory_used': '46,214'}\"\n",
      "08:07:13 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.031982825857824734'.\"\n",
      "08:07:13 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "08:07:14 | INFO | \"  (Delayed('int-e47a2c12-cacd-4dfd-95da-0675ec9df027'), 513) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "08:07:14 | INFO | \"  0:00:01.505814 <- Total (df_posts_agg_c) posts+comments+subs agg time elapsed\"\n",
      "08:07:14 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.19%', 'memory_used': '46,164'}\"\n",
      "08:07:14 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.03194822289567276'.\"\n",
      "08:07:14 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "08:07:14 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "08:07:14 | INFO | \"A - posts only\"\n",
      "08:07:15 | INFO | \"  (Delayed('int-bc3413f4-ac22-47b9-ac6e-5d2178183c6f'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
      "08:07:15 | INFO | \"B - posts + comments\"\n",
      "08:07:15 | INFO | \"  (Delayed('int-c02846a9-5665-45eb-ba3f-3e73b2fe005a'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
      "08:07:15 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "08:07:16 | INFO | \"  (Delayed('int-ca0bf5db-79a3-47f1-b27f-852f4a96f8e2'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "08:07:16 | INFO | \"  0:00:01.403428 <- Total for ALL subreddit-level agg time elapsed\"\n",
      "08:07:16 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.20%', 'memory_used': '46,167'}\"\n",
      "08:07:16 | ERROR | \"Error logging CPU & RAM info\n",
      " Changing param values is not allowed. Param with key='memory_used_percent' was already logged with value='0.0015370635787905988' for run ID='0b7a295ef133482390384b5cc508cb58'. Attempted logging new value '0.031950299073401876'.\"\n",
      "08:07:16 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "08:07:16 | INFO | \"  Saving config to local path...\"\n",
      "08:07:16 | INFO | \"  Logging config to mlflow...\"\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]08:07:16 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "08:07:16 | INFO | \"  Saving locally...\"\n",
      "08:07:16 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "  0%|                                                     | 0/5 [04:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36mrun_aggregation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m#    creating clusters w/o having to wait for distances to be computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# TODO(djb): log and save C) before B or A (C is what gives the best outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_and_log_aggregate_and_similarity_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_ram_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_memory_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36m_save_and_log_aggregate_and_similarity_dfs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_sub_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m                     \u001b[0mwrite_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m                 )\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/utils/mlflow_logger.py\u001b[0m in \u001b[0;36msave_pd_df_to_parquet_in_chunks\u001b[0;34m(df, path, target_mb_size, write_index)\u001b[0m\n\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Saving existing dask df as parquet...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;31m# if it's a dask df, simply save as is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Don't log partition size, this might add overhead/time that's a waste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4371\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mderived_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/dataframe/io/parquet/core.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mcompute_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1986\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m             )\n\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             return sync(\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m             )\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()\n",
    "try:\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "job_agg_test = AggregateEmbeddings(\n",
    "    run_name=f\"sample_test_lc_false-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_test_sample_lc_false.config_flat\n",
    ")\n",
    "job_agg_test.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a23d591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:15:13 | INFO | \"Logging log-file to mlflow...\"\n"
     ]
    }
   ],
   "source": [
    "job_agg_test._send_log_file_to_mlflow()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39a8fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_agg_test._save_and_log_aggregate_and_similarity_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cc5eee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2828"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7a0be",
   "metadata": {},
   "source": [
    "# Check output dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6068d7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vars(job_agg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08487294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_dfs2 = {k: v for k, v in vars(job_agg_test).items() if 'df_' in k}\n",
    "\n",
    "\n",
    "# for k2, df_2 in tqdm(d_dfs2.items()):\n",
    "#     print(f\"\\n{k2}\")\n",
    "#     try:\n",
    "#         print(f\"  {df_2.shape} <- df shape\")\n",
    "#         print(f\"  {df_2.npartitions} <- dask partitions\")\n",
    "#         # print(f\"{get_dask_df_shape(df_2)} <- df.shape\")\n",
    "#         # print(f\"  {df_2.memory_usage(deep=True).sum() / 1048576:4,.1f} MB <- Memory usage\")\n",
    "#         if any(['meta' in k2, '_v_' in k2]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             pass\n",
    "# #             display(df_2.iloc[:5, :15])\n",
    "\n",
    "#     except (TypeError, AttributeError):\n",
    "#         if isinstance(df_2, pd.DataFrame):\n",
    "#             print(f\"  {df_2.shape} <- df shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063edc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120f3750",
   "metadata": {},
   "source": [
    "## VM size notes\n",
    "\n",
    "`614 GB` of RAM is not enough for 40 million posts...\n",
    "\n",
    "VM & cluster set up:\n",
    "```bash\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```\n",
    "\n",
    "Traceback:\n",
    "\n",
    "```bash\n",
    "23:17:48 | INFO | \"      775,092 <- Comments that DON'T need to be averaged\"\n",
    "23:17:48 | INFO | \"   39,126,876 <- Comments that need to be averaged\"\n",
    "23:17:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "---------------------------------------------------------------------------\n",
    "KilledWorker                              Traceback (most recent call last)\n",
    "<timed exec> in <module>\n",
    "\n",
    "/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py in run_aggregation(self)\n",
    "    249         # - up-votes\n",
    "    250         # ---\n",
    "--> 251         self._agg_comments_to_post_level()\n",
    "...\n",
    "KilledWorker: (\"('dataframe-groupby-sum-agg-849e94fd54a49f8ed34330862f20cb9d', 0)\", <WorkerState 'tcp://127.0.0.1:41351', name: 6, memory: 0, processing: 1>)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97aff1",
   "metadata": {},
   "source": [
    "### time profiling\n",
    "\n",
    "inputs:\n",
    "``` python\n",
    "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
    "n_sample_posts_files: \t5\n",
    "n_sample_comments_files: \t10\n",
    "\n",
    "aggregate_params:\n",
    "  min_comment_text_len: \t10\n",
    "  agg_comments_to_post_weight_col: \tNone\n",
    "  agg_post_to_subreddit_weight_col: \tNone\n",
    "  agg_post_post_weight: \t70\n",
    "  agg_post_comment_weight: \t20\n",
    "  agg_post_subreddit_desc_weight: \t10\n",
    "```\n",
    "\n",
    "VM & cluster set up:\n",
    "```\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14447047",
   "metadata": {},
   "source": [
    "### Filtered/selected logs\n",
    "\n",
    "Overview:\n",
    "\n",
    "| Time/ETA | Step | Notes |\n",
    "| --- | --- | --- |\n",
    "| `0:11:43` minutes | load raw embeddings (w/o caching) | |\n",
    "| `0:03:15` minutes | Load metadata (w/o caching):  |  | \n",
    "| `0:04:30` minutes | Aggegation steps (all) | Note that this might only be the time to create the dag, not necessarily the time to actually compute the data | \n",
    "| `0:37:24` minutes | Calculate similarities  |  | \n",
    "| `1:30:00` HOURS | Saving & logging files | Saving alone could take more than 1 hour... mand I'd forgotten about this | \n",
    "|  |  |  | \n",
    "\n",
    "\n",
    "Note that there's very different ETAs for saving each DF, the first 2 are really large and take a long time. The last few are smaller, so the time estimates from `tqdm` can vary a ton:\n",
    "```bash\n",
    "3/11 [40:25<1:14:34, 559.33s/it]   27%\n",
    "9/11 [50:22<03:22, 101.02s/it]     82% \n",
    "11/11 [1:30:11<00:00, 750.74s/it] 100%\n",
    "```\n",
    "\n",
    "\n",
    "Getting shape of `dask df` is taking almost half of the saving time!\n",
    "\n",
    "**TODO: REMOVE** logging df shape for now to save a ton of time!\n",
    "\n",
    "```bash\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"                                   # get_df_shape() starts here...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"             # get df_shape() ends here, ABOUT 40 MINUTES!\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"                       # In contrast, SAVING the dask df only takes about 20 MINUTES!\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"    # And logging the dfs up to GCS only takes about 3 seconds?!\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "21:33:14 | INFO | \"Keeping index intact...\"\n",
    "21:33:14 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:15 | INFO | \"   185.4 MB <- Memory usage\"\n",
    "21:33:15 | INFO | \"       5\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
    "21:33:19 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:22 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
    "21:33:22 | INFO | \"Saving locally...\"\n",
    "21:33:24 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:35 | INFO | \"  10,391.8 MB <- Memory usage\"\n",
    "21:33:35 | INFO | \"     139\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
    "21:33:55 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:34:30 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c4290",
   "metadata": {},
   "source": [
    "More details in log file:\n",
    "\n",
    "`logs/AggregateEmbeddings/2021-10-05_195710-sample_test_lc_false-2021-10-05_195710.log`\n",
    "\n",
    "\n",
    "```bash\n",
    "# load raw embeddings (w/o caching): 11:43 minutes\n",
    "# ---\n",
    "20:01:19 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
    "40/40 [07:29<00:00, 8.17s/it]\n",
    "20:08:49 | INFO | \"  Parquet files found: 5\"\n",
    "20:08:49 | INFO | \"  Keep only comments for posts with embeddings\"\n",
    "20:08:54 | INFO | \"  0:11:43.326935 <- Total raw embeddings load time elapsed\"\n",
    "\n",
    "\n",
    "# Load metadata (w/o caching): 3:15 minutes\n",
    "# ---\n",
    "20:08:54 | INFO | \"-- Start _load_metadata() method --\"\n",
    "20:08:54 | INFO | \"Loading POSTS metadata...\"\n",
    "\n",
    "20:10:39 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
    "100%\n",
    "59/59 [01:30<00:00, 1.43s/it]\n",
    "20:12:10 | INFO | \"  (Delayed('int-e6188e6d-6319-487d-b054-bea8a30d912b'), 7) <- Raw META COMMENTS shape\"\n",
    "20:12:10 | INFO | \"  0:03:15.218880 <- Total metadata loading time elapsed\"\n",
    "\n",
    "\n",
    "# Aggegation steps (all): 4:30 minutes\n",
    "#   Note that this might only be the time to create the dag, not necessarily the time to actually compute the data\n",
    "20:12:10 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
    "20:12:10 | INFO | \"Getting count of comments per post...\"\n",
    "20:12:39 | INFO | \"Filtering which comments need to be averaged...\"\n",
    "20:13:23 | INFO | \"       22,197 <- Comments that DON'T need to be averaged\"\n",
    "20:13:23 | INFO | \"    1,087,458 <- Comments that need to be averaged\"\n",
    "20:13:28 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "20:13:57 | INFO | \"      191,558 |  514 <- df_v_com_agg SHAPE\"\n",
    "20:13:57 | INFO | \"  0:01:46.878385 <- Total comments to post agg loading time elapsed\"\n",
    "20:13:57 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
    "20:13:59 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
    "...\n",
    "20:16:38 | INFO | \"A - posts only\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-3ee084d4-434c-4a38-aeb1-185b50648908'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
    "20:16:39 | INFO | \"B - posts + comments\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-6bd21f72-fc1d-495c-987a-1da6e4a18683'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
    "20:16:39 | INFO | \"C - posts + comments + sub descriptions\"\n",
    "20:16:40 | INFO | \"  (Delayed('int-59c54047-7ac8-49cb-81b3-478b4ae5b60e'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
    "20:16:40 | INFO | \"  0:00:01.507065 <- Total for ALL subreddit-level agg time elapsed\"\n",
    "\n",
    "\n",
    "# Calculate similarities 37:24 minutes\n",
    "20:16:40 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
    "20:16:40 | INFO | \"A...\"\n",
    "20:16:56 | INFO | \"  (4924, 4924) <- df_subs_agg_a_similarity.shape\"\n",
    "20:17:21 | INFO | \"Merge distance + metadata...\"\n",
    "20:17:59 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
    "20:18:10 | INFO | \"  (24240852, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
    "20:18:10 | INFO | \"  (98480, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
    "...\n",
    "20:54:04 | INFO | \"  0:37:24.347689 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
    "\n",
    "\n",
    "# *** Saving & logging file: WTF? Saving alone could take more than 2 hours!! WTF?!!  ***\n",
    "20:54:04 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
    "20:54:04 | INFO | \"  Saving config to local path...\"\n",
    "20:54:04 | INFO | \"  Logging config to mlflow...\"\n",
    "*** 3/11 [40:25<1:14:34, 559.33s/it]  27%   ***\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"\n",
    "...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ef38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
