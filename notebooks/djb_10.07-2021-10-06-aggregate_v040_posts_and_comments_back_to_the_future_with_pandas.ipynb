{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8fee7dd",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "2021-10-06:\n",
    "We're going back to pandas now that I have the VM machine with a ton of RAM.\n",
    "\n",
    "There might be some tweaks needed to batch a few subreddits at a time, but at least we can get more consistent state/progress than with `dask`.\n",
    "\n",
    "---\n",
    "2021-10-06:\n",
    "The job with dask failed silently - even with 3+ TB of RAM.  `Dask` was reporting that saving was complete - but it only saved one `parquet` file instead of hundreds of files.\n",
    "\n",
    "New direction: now that I have access to a large VM, I might as well try to go back and do the calculations in memory (in pandas).\n",
    "\n",
    "\n",
    "-- \n",
    "2021-10-05:\n",
    "I ran into memory errors with 600GB or RAM, so here's a try with 1.4TB... if this doesn't work. Then I don't know what will...\n",
    "\n",
    "---\n",
    "\n",
    "2021-08-10: Finally completed testing with sampling <= 10 files. Now ready to run process on full data!\n",
    "\n",
    "Ended up doing it all in dask + pandas + numpy because of problems installing `cuDF`.\n",
    "\n",
    "---\n",
    "2021-08-02: Now that I'm processing millions of comments and posts, I need to re-write the functions to try to do some work in parallel and reduce the amount of data loaded in RAM.\n",
    "\n",
    "- `Dask` seems like a great option to load data and only compute some of it as needed.\n",
    "- `cuDF` could be a way to speed up some computation using GPUs\n",
    "- `Dask-delayed` could be a way to create a task DAG lazily before computing all the aggregates.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In notebook 09 I combined embeddings from posts & subreddits (`djb_09.00-combine_post_and_comments_and_visualize_for_presentation.ipynb`).\n",
    "\n",
    "In this notebook I'll be testing functions that include mlflow so that it's easier to try a lot of different weights to find better respresentations.\n",
    "\n",
    "Take embeddings created by other models & combine them:\n",
    "```\n",
    "new post embeddings = post + comments + subreddit description\n",
    "\n",
    "new subreddit embeddings = new posts (weighted by post length or upvotes?)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8fff",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0cfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602d4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba329c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd88b4",
   "metadata": {},
   "source": [
    "# Set sqlite database as MLflow URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f95300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ba927",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8274bfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d0a92",
   "metadata": {},
   "source": [
    "## Get runs that we can use for embeddings aggregation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92bda91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 301 ms, sys: 5.69 ms, total: 307 ms\n",
      "Wall time: 306 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_mlf_runs =  mlf.search_all_runs(experiment_ids=[13, 14, 15, 16])\n",
    "df_mlf_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746c47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_finished = df_mlf_runs['status'] == 'FINISHED'\n",
    "mask_output_over_1M_rows = (\n",
    "    (df_mlf_runs['metrics.df_vect_posts_rows'] >= 1e5) |\n",
    "    (df_mlf_runs['metrics.df_vect_comments'] >= 1e5)\n",
    ")\n",
    "# df_mlf_runs[mask_finished].shape\n",
    "\n",
    "df_mlf_use_for_agg = df_mlf_runs[mask_output_over_1M_rows]\n",
    "df_mlf_use_for_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0a22fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_5a9b1_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >run id</th>        <th class=\"col_heading level0 col1\" >experiment id</th>        <th class=\"col_heading level0 col2\" >start time</th>        <th class=\"col_heading level0 col3\" >metrics.df vect comments</th>        <th class=\"col_heading level0 col4\" >metrics.vectorizing time minutes comments</th>        <th class=\"col_heading level0 col5\" >metrics.total comment files processed</th>        <th class=\"col_heading level0 col6\" >metrics.vectorizing time minutes full function</th>        <th class=\"col_heading level0 col7\" >params.tf batch inference rows</th>        <th class=\"col_heading level0 col8\" >params.n sample comment files</th>        <th class=\"col_heading level0 col9\" >params.n comment files slice start</th>        <th class=\"col_heading level0 col10\" >params.n comment files slice end</th>        <th class=\"col_heading level0 col11\" >tags.mlflow.runName</th>        <th class=\"col_heading level0 col12\" >tags.model version</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_5a9b1_level0_row0\" class=\"row_heading level0 row0\" >47</th>\n",
       "                        <td id=\"T_5a9b1_row0_col0\" class=\"data row0 col0\" >deb3454ece2a4a8d8e4149c2d8494c0d</td>\n",
       "                        <td id=\"T_5a9b1_row0_col1\" class=\"data row0 col1\" >14</td>\n",
       "                        <td id=\"T_5a9b1_row0_col2\" class=\"data row0 col2\" >2021-10-05 01:44:32.386000+00:00</td>\n",
       "                        <td id=\"T_5a9b1_row0_col3\" class=\"data row0 col3\" >10,121,046.00</td>\n",
       "                        <td id=\"T_5a9b1_row0_col4\" class=\"data row0 col4\" >39.11</td>\n",
       "                        <td id=\"T_5a9b1_row0_col5\" class=\"data row0 col5\" >15.00</td>\n",
       "                        <td id=\"T_5a9b1_row0_col6\" class=\"data row0 col6\" >45.94</td>\n",
       "                        <td id=\"T_5a9b1_row0_col7\" class=\"data row0 col7\" >3200</td>\n",
       "                        <td id=\"T_5a9b1_row0_col8\" class=\"data row0 col8\" >15</td>\n",
       "                        <td id=\"T_5a9b1_row0_col9\" class=\"data row0 col9\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row0_col10\" class=\"data row0 col10\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row0_col11\" class=\"data row0 col11\" >comments_batch_01-2021-10-05_014431</td>\n",
       "                        <td id=\"T_5a9b1_row0_col12\" class=\"data row0 col12\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5a9b1_level0_row1\" class=\"row_heading level0 row1\" >48</th>\n",
       "                        <td id=\"T_5a9b1_row1_col0\" class=\"data row1 col0\" >5f10cd75334142168a6ebb787e477c1f</td>\n",
       "                        <td id=\"T_5a9b1_row1_col1\" class=\"data row1 col1\" >14</td>\n",
       "                        <td id=\"T_5a9b1_row1_col2\" class=\"data row1 col2\" >2021-10-05 00:22:20.334000+00:00</td>\n",
       "                        <td id=\"T_5a9b1_row1_col3\" class=\"data row1 col3\" >13,558,304.00</td>\n",
       "                        <td id=\"T_5a9b1_row1_col4\" class=\"data row1 col4\" >47.64</td>\n",
       "                        <td id=\"T_5a9b1_row1_col5\" class=\"data row1 col5\" >20.00</td>\n",
       "                        <td id=\"T_5a9b1_row1_col6\" class=\"data row1 col6\" >57.33</td>\n",
       "                        <td id=\"T_5a9b1_row1_col7\" class=\"data row1 col7\" >4200</td>\n",
       "                        <td id=\"T_5a9b1_row1_col8\" class=\"data row1 col8\" >20</td>\n",
       "                        <td id=\"T_5a9b1_row1_col9\" class=\"data row1 col9\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row1_col10\" class=\"data row1 col10\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row1_col11\" class=\"data row1 col11\" >comments_batch_01-2021-10-05_002219</td>\n",
       "                        <td id=\"T_5a9b1_row1_col12\" class=\"data row1 col12\" >0.4.0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5a9b1_level0_row2\" class=\"row_heading level0 row2\" >52</th>\n",
       "                        <td id=\"T_5a9b1_row2_col0\" class=\"data row2 col0\" >9a27f9a72cf348c98d50f486abf3b009</td>\n",
       "                        <td id=\"T_5a9b1_row2_col1\" class=\"data row2 col1\" >13</td>\n",
       "                        <td id=\"T_5a9b1_row2_col2\" class=\"data row2 col2\" >2021-10-04 22:21:46.401000+00:00</td>\n",
       "                        <td id=\"T_5a9b1_row2_col3\" class=\"data row2 col3\" >1,286,661.00</td>\n",
       "                        <td id=\"T_5a9b1_row2_col4\" class=\"data row2 col4\" >3.93</td>\n",
       "                        <td id=\"T_5a9b1_row2_col5\" class=\"data row2 col5\" >2.00</td>\n",
       "                        <td id=\"T_5a9b1_row2_col6\" class=\"data row2 col6\" >5.03</td>\n",
       "                        <td id=\"T_5a9b1_row2_col7\" class=\"data row2 col7\" >6000</td>\n",
       "                        <td id=\"T_5a9b1_row2_col8\" class=\"data row2 col8\" >2</td>\n",
       "                        <td id=\"T_5a9b1_row2_col9\" class=\"data row2 col9\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row2_col10\" class=\"data row2 col10\" >None</td>\n",
       "                        <td id=\"T_5a9b1_row2_col11\" class=\"data row2 col11\" >posts_as_comments_full_text-2021-10-04_222146</td>\n",
       "                        <td id=\"T_5a9b1_row2_col12\" class=\"data row2 col12\" >None</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f41b3c21550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_multiple_vals = df_mlf_use_for_agg.columns[df_mlf_use_for_agg.nunique(dropna=False) > 1]\n",
    "# len(cols_with_multiple_vals)\n",
    "\n",
    "style_df_numeric(\n",
    "    df_mlf_use_for_agg\n",
    "    [cols_with_multiple_vals]\n",
    "    .drop(['artifact_uri', 'end_time',\n",
    "           # 'start_time',\n",
    "           ], \n",
    "          axis=1)\n",
    "    .dropna(axis='columns', how='all')\n",
    "    .iloc[:, :30]\n",
    "    ,\n",
    "    rename_cols_for_display=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e4dad",
   "metadata": {},
   "source": [
    "# Load configs for aggregation jobs\n",
    "\n",
    "`n_sample_comments_files` and `n_sample_posts_files` allow us to only load a few files at a time (e.g., 2 instead of 50) to test the process end-to-end.\n",
    "\n",
    "---\n",
    "Note that by default `hydra` is a cli tool. If we want to call use it in jupyter, we need to manually initialize configs & compose the configuration. See my custom function `load_config_agg_jupyter`. Also see:\n",
    "- [Notebook with `Hydra` examples in a notebook](https://github.com/facebookresearch/hydra/blob/master/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb).\n",
    "- [Hydra docs, Hydra in Jupyter](https://hydra.cc/docs/next/advanced/jupyter_notebooks/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092ed8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=4',     # \n",
    "               'n_sample_comments_files=4',  # 6 is limit for logging unique counts at comment level\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "config_full_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "               'n_sample_posts_files=null', \n",
    "               'n_sample_comments_files=null',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "# config_full_lc_true = AggregateEmbeddingsConfig(\n",
    "#     config_path=\"../config\",\n",
    "#     config_name='aggregate_embeddings',\n",
    "#     overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "#                'n_sample_posts_files=null', \n",
    "#                'n_sample_comments_files=null',\n",
    "#                'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_true',\n",
    "#               ]\n",
    "# )\n",
    "# pprint(config_test_sample_lc_false.config_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608682dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_test_sample_lc_false.config_flat,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4877d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_configs = pd.DataFrame(\n",
    "    [\n",
    "        config_test_sample_lc_false.config_flat,\n",
    "        # config_test_full_lc_false.config_flat,\n",
    "        config_full_lc_false.config_flat,\n",
    "        # config_full_lc_true.config_flat,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c238f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>comments_uuid</th>\n",
       "      <th>mlflow_experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comments_vectorized_mlflow_uuids       posts_vectorized_mlflow_uuids posts_vectorized_mlflow_uuids_lowercase subreddit_meta_vectorized_mlflow_uuids subreddit_meta_vectorized_mlflow_uuids_lowercase  \\\n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "\n",
       "                                                          comments_uuid                 mlflow_experiment  \n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  v0.4.0_use_multi_aggregates_test  \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]       v0.4.0_use_multi_aggregates  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can't use (df_configs.nunique(dropna=False) > 1)\n",
    "#  because when a col's content is a list or something unhashable, we get an error\n",
    "#  so instead we'll check each column individually\n",
    "\n",
    "# cols_with_diffs_config = df_configs.columns[df_configs.nunique(dropna=False) > 1]\n",
    "cols_with_diffs_config = list()\n",
    "for c_ in df_configs.columns:\n",
    "    try:\n",
    "        if df_configs[c_].nunique() > 1:\n",
    "            cols_with_diffs_config.append(c_)\n",
    "    except TypeError:\n",
    "        cols_with_diffs_config.append(c_)\n",
    "        \n",
    "\n",
    "df_configs[cols_with_diffs_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6f0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(config_test_sample_lc_false.config_flat, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ef245",
   "metadata": {},
   "source": [
    "# Run Full data with `lower_case=False`\n",
    "\n",
    "The logic for sampling files and download/`caching` files locally lives in the `mlf` custom function.\n",
    "\n",
    "Caching can save 9+ minutes if we try to download the files from GCS every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a170aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates\n",
      "n_sample_posts_files: \tNone\n",
      "n_sample_comments_files: \tNone\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t2\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tTrue\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_full_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c2374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:53:34 | INFO | \"== Start run_aggregation() method ==\"\n",
      "09:53:34 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "09:53:35 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "09:53:35 | INFO | \"cpu_count: 160\"\n",
      "09:53:35 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '1.98%', 'memory_total': '3,874,634', 'memory_used': '76,640', 'memory_free': '3,759,068'}\"\n",
      "09:53:35 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-12_095335-agg_full_lc_false-2021-10-12_095334\"\n",
      "09:53:35 | INFO | \"  Saving config to local path...\"\n",
      "09:53:35 | INFO | \"  Logging config to mlflow...\"\n",
      "09:53:36 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "09:53:36 | INFO | \"Loading subreddit description embeddings...\"\n",
      "09:53:36 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_subreddits_description\"\n",
      "100%|##########################################| 5/5 [00:00<00:00, 13733.80it/s]\n",
      "09:53:37 | INFO | \"  Parquet files found: 2\"\n",
      "09:53:37 | INFO | \"      19,262 |  513 <- Raw vectorized subreddit description shape\"\n",
      "09:53:37 | INFO | \"  Unique check for subreddit description...\"\n",
      "09:53:38 | INFO | \"Loading POSTS embeddings...\"\n",
      "09:53:38 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_posts\"\n",
      "100%|########################################| 28/28 [00:00<00:00, 24350.10it/s]\n",
      "09:53:38 | INFO | \"  Parquet files found: 27\"\n",
      "09:53:39 | INFO | \"  Getting df_v_posts.shape ...\"\n",
      "09:54:18 | INFO | \"   8,439,672 |  514 <- Raw POSTS shape\"\n",
      "09:54:18 | INFO | \"  Checking that posts are unique...\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    job_agg1._send_log_file_to_mlflow()\n",
    "    mlflow.end_run(\"FAILED\")\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    \n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "\n",
    "\n",
    "job_agg1 = AggregateEmbeddings(\n",
    "    run_name=f\"agg_full_lc_false-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_full_lc_false.config_flat\n",
    ")\n",
    "job_agg1.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_agg1._send_log_file_to_mlflow()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee37894",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5836f",
   "metadata": {},
   "source": [
    "# Run full data, `lower_case=True`\n",
    "\n",
    "Looks like the problem I ran into with the file being corrupted might've been a problem with downloading the file(s). Fix: delete the local cache and download the files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb66bbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:51 | INFO | \"== Start run_aggregation() method ==\"\n",
      "15:47:51 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "15:47:52 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-08-10_154752-full_lc_true-2021-08-10_154751\"\n",
      "15:47:52 | INFO | \"  Saving config to local path...\"\n",
      "15:47:52 | INFO | \"  Logging config to mlflow...\"\n",
      "15:47:52 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "15:47:52 | INFO | \"Loading subreddit description embeddings...\"\n",
      "15:47:53 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_subreddits_description\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7e6e4097cc4648a0a6dcf405072654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:54 | INFO | \"  Reading 1 files\"\n",
      "15:47:55 | INFO | \"       3,767 |  513 <- Raw vectorized subreddit description shape\"\n",
      "15:47:56 | INFO | \"Loading POSTS embeddings...\"\n",
      "15:47:57 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_posts\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b70848d70c4886834fc853336b4b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:44 | INFO | \"  Reading 48 files\"\n",
      "15:48:47 | INFO | \"   1,649,929 |  514 <- Raw POSTS shape\"\n",
      "15:48:51 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "15:48:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_comments\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ab07ab988b41b9b0d92315679c18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:48 | INFO | \"  Reading 37 files\"\n",
      "15:54:49 | INFO | \"  0:06:56.293258 <- Total raw embeddings load time elapsed\"\n",
      "15:54:49 | INFO | \"-- Start _load_metadata() method --\"\n",
      "15:54:49 | INFO | \"Loading POSTS metadata...\"\n",
      "15:54:49 | INFO | \"Reading raw data...\"\n",
      "15:54:49 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59eb9efacfd0489f806b804fd167453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:51 | INFO | \"  Applying transformations...\"\n",
      "15:54:52 | INFO | \"  (1649929, 14) <- Raw META POSTS shape\"\n",
      "15:54:52 | INFO | \"Loading subs metadata...\"\n",
      "15:54:52 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "15:54:52 | INFO | \"Reading raw data...\"\n",
      "15:54:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98ebb0befa4f09aa0cc006cb99f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:53 | INFO | \"  Applying transformations...\"\n",
      "15:54:54 | INFO | \"  (3767, 38) <- Raw META subreddit description shape\"\n",
      "15:54:54 | INFO | \"Loading COMMENTS metadata...\"\n",
      "15:54:54 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-07-09\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e52ad01a47422bbde292fb39492af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:55 | INFO | \"  (Delayed('int-11aa2518-d088-4702-bee1-c90e9c40927d'), 7) <- Raw META COMMENTS shape\"\n",
      "15:54:55 | INFO | \"  0:00:05.773888 <- Total metadata loading time elapsed\"\n",
      "15:54:55 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "15:54:55 | INFO | \"Getting count of comments per post...\"\n",
      "15:55:17 | WARNING | \"Error creating summary of comments per post.\n",
      "'<=' not supported between instances of 'NoneType' and 'int'\"\n",
      "15:55:18 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "15:56:48 | INFO | \"      126,642 <- Comments that DON'T need to be averaged\"\n",
      "15:56:48 | INFO | \"   19,041,512 <- Comments that need to be averaged\"\n",
      "15:56:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "15:59:15 | INFO | \"      979,701 |  514 <- df_v_com_agg SHAPE\"\n",
      "15:59:15 | INFO | \"  0:04:20.021986 <- Total comments to post agg loading time elapsed\"\n",
      "15:59:15 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
      "15:59:17 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
      "15:59:17 | INFO | \"  (Delayed('int-2f2dd3eb-5832-498e-ac7f-0f8fc90cbef9'), 513) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
      "15:59:17 | INFO | \"Concat aggregated comments+posts with posts-without comments\"\n",
      "16:09:32 | INFO | \"    1,649,929 |  514 <- df_posts_agg_b shape after aggregation\"\n",
      "16:09:32 | INFO | \"  0:10:16.855088 <- Total (df_posts_agg_b) posts & comments agg time elapsed\"\n",
      "16:09:32 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-2afffaf2-f55e-4a71-b7a9-43db8c7e9419'), 513) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "16:09:35 | INFO | \"  0:00:02.606942 <- Total (df_posts_agg_c) posts+comments+subs agg time elapsed\"\n",
      "16:09:35 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "16:09:35 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "16:09:35 | INFO | \"A - posts only\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-c0cdc12a-7599-4f26-8877-628f56e69ca7'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
      "16:09:35 | INFO | \"B - posts + comments\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-4a88fcb6-f299-4f12-96d0-1c52f2200024'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
      "16:09:35 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "16:09:36 | INFO | \"  (Delayed('int-2c3b142e-c4c6-44e7-8f3b-5b7bafa6f4a0'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "16:09:36 | INFO | \"  0:00:01.266842 <- Total for ALL subreddit-level agg time elapsed\"\n",
      "16:09:36 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
      "16:09:36 | INFO | \"A...\"\n",
      "16:09:48 | INFO | \"  (3767, 3767) <- df_subs_agg_a_similarity.shape\"\n",
      "16:10:07 | INFO | \"Merge distance + metadata...\"\n",
      "16:10:37 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:10:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:10:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:10:50 | INFO | \"B...\"\n",
      "16:29:57 | INFO | \"  (3767, 3767) <- df_subs_agg_b_similarity.shape\"\n",
      "16:30:16 | INFO | \"Merge distance + metadata...\"\n",
      "16:30:46 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:30:54 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:30:54 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:31:00 | INFO | \"C...\"\n",
      "17:17:45 | INFO | \"  (3767, 3767) <- df_subs_agg_c_similarity.shape\"\n",
      "17:18:06 | INFO | \"Merge distance + metadata...\"\n",
      "17:18:36 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "17:18:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "17:18:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "17:18:49 | INFO | \"  1:09:13.049794 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
      "17:18:49 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "17:18:49 | INFO | \"  Saving config to local path...\"\n",
      "17:18:49 | INFO | \"  Logging config to mlflow...\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f472b6e6ac4d91b6d55cd455fbfc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:18:50 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "17:18:50 | INFO | \"Saving locally...\"\n",
      "17:42:53 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:06:23 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:25 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
      "18:06:25 | INFO | \"Saving locally...\"\n",
      "18:06:25 | INFO | \"Keeping index intact...\"\n",
      "18:06:25 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:25 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:06:25 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:06:29 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:31 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
      "18:06:31 | INFO | \"Saving locally...\"\n",
      "18:06:33 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:40 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:06:40 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "18:06:53 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:07:16 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
      "18:07:16 | INFO | \"Saving locally...\"\n",
      "18:17:47 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:28:42 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:44 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity **\"\n",
      "18:28:44 | INFO | \"Saving locally...\"\n",
      "18:28:44 | INFO | \"Keeping index intact...\"\n",
      "18:28:44 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:44 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:28:44 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:28:48 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:50 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity_pair **\"\n",
      "18:28:50 | INFO | \"Saving locally...\"\n",
      "18:28:52 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:59 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:28:59 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# mlflow.end_run(\"FAILED\")\n",
    "# gc.collect()\n",
    "# try:\n",
    "#     # run setup_logging() to remove logging to the file of a failed job\n",
    "#     setup_logging()\n",
    "    \n",
    "#     del job_agg2\n",
    "#     del d_dfs2\n",
    "# except NameError:\n",
    "#     pass\n",
    "# gc.collect()\n",
    "\n",
    "# job_agg2 = AggregateEmbeddings(\n",
    "#     run_name=f\"full_lc_true-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     **config_full_lc_true.config_flat\n",
    "# )\n",
    "# job_agg2.run_aggregation()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcfabf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c0ff4",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27098cad",
   "metadata": {},
   "source": [
    "# Run test with `lower_case=False\n",
    "\n",
    "Sample only a few files in comments/ posts to make sure that job completes even when we're testing new code/logic.\n",
    "\n",
    "Limit to only 2 files of each kind to get minimum test to run end to end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35eeffe",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdd70c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run setup_logging() to remove logging to the file of a failed job\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed5679e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:20:03 | INFO | \"info test\"\n",
      "16:20:03 | WARNING | \"warning message\"\n",
      "16:20:03 | ERROR | \"error message\"\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"debug test\")\n",
    "logging.info(\"info test\")\n",
    "logging.warning(\"warning message\")\n",
    "logging.error(\"error message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbc9ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=2',     # \n",
    "               'n_sample_comments_files=4',  # 6 is limit for logging unique counts at comment level\n",
    "               'calculate_similarites=false',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce7a6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
      "n_sample_posts_files: \t2\n",
      "n_sample_comments_files: \t4\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t2\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tFalse\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_test_sample_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79dfbd",
   "metadata": {},
   "source": [
    "## Run test job/config\n",
    "\n",
    "\n",
    "TODO: how to vectorize or run this in parallel?\n",
    "\n",
    "\n",
    "It took around 9.5 minutes to go through 465k posts.\n",
    "```bash\n",
    "100%|| 464967/464967 [09:21<00:00, 827.91it/s]\n",
    "22:14:59 | INFO | \"  (464967, 512) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c7311c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:03:14 | INFO | \"Logging log-file to mlflow...\"\n",
      "22:03:15 | INFO | \"== Start run_aggregation() method ==\"\n",
      "22:03:15 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "22:03:17 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "22:03:17 | INFO | \"cpu_count: 160\"\n",
      "22:03:18 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.08%', 'memory_total': '3,874,634', 'memory_used': '119,506', 'memory_free': '3,721,142'}\"\n",
      "22:03:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-11_220318-sample_test_lc_false_pd-2021-10-11_220315\"\n",
      "22:03:18 | INFO | \"  Saving config to local path...\"\n",
      "22:03:18 | INFO | \"  Logging config to mlflow...\"\n",
      "22:03:19 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "22:03:19 | INFO | \"Raw subreddit embeddings pre-loaded\"\n",
      "22:03:19 | INFO | \"      19,262 | 514 <- Raw vectorized subreddit description shape\"\n",
      "22:03:19 | INFO | \"  Unique check for subreddit description...\"\n",
      "22:03:19 | INFO | \"POSTS embeddings pre-loaded\"\n",
      "22:03:27 | INFO | \"   8,439,672 |  515 <- Raw POSTS shape\"\n",
      "22:03:27 | INFO | \"  Checking that posts are unique...\"\n",
      "22:03:34 | INFO | \"COMMENTS embeddings pre-loaded\"\n",
      "22:03:36 | INFO | \"   2,649,171 |  516 <- Raw COMMENTS shape\"\n",
      "22:03:36 | INFO | \"  Keep only comments for posts with embeddings\"\n",
      "22:03:48 | INFO | \"   2,649,171 |  516 <- COMMENTS shape, after keeping only comments to loaded posts\"\n",
      "22:03:52 | INFO | \"  0:00:32.819404 <- Total raw embeddings load time elapsed\"\n",
      "22:03:54 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.65%', 'memory_used': '141,559'}\"\n",
      "22:03:54 | INFO | \"-- Start _load_metadata() method --\"\n",
      "22:03:54 | INFO | \"Posts META pre-loaded\"\n",
      "22:03:54 | INFO | \"  (8439672, 15) <- Raw META POSTS shape\"\n",
      "22:03:54 | INFO | \"Loading subs metadata...\"\n",
      "22:03:54 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "22:03:54 | INFO | \"Reading raw data...\"\n",
      "22:03:54 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-09-24\"\n",
      "100%|#################################| 1/1 [00:00<00:00, 6647.07it/s]\n",
      "22:03:54 | INFO | \"  Applying transformations...\"\n",
      "22:04:03 | INFO | \"  (19262, 91) <- Raw META subreddit description shape\"\n",
      "22:04:03 | INFO | \"Comments META pre-loaded\"\n",
      "22:04:03 | INFO | \"  (39901968, 8) <- Raw META COMMENTS shape\"\n",
      "22:04:03 | INFO | \"  0:00:09.020872 <- Total metadata loading time elapsed\"\n",
      "22:04:05 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.65%', 'memory_used': '141,605'}\"\n",
      "22:04:05 | INFO | \"2 <- Removing comments shorter than 2 characters.\"\n",
      "22:04:09 | INFO | \"  (2649171, 516) <- df_v_comments.shape AFTER removing short comments\"\n",
      "22:04:10 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "22:04:10 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "22:04:41 | INFO | \"  (464967, 514) <- df_v_com_agg shape after aggregation\"\n",
      "22:04:41 | WARNING | \"  TODO(djb): DELETE df_v_com (raw) to free up memory...\"\n",
      "22:04:41 | INFO | \"  0:00:30.941788 <- Total comments to post agg loading time elapsed\"\n",
      "22:04:42 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '3.68%', 'memory_used': '142,480'}\"\n",
      "22:04:42 | INFO | \"-- Start _agg_posts_and_comments_to_post_level() method --\"\n",
      "22:04:43 | INFO | \"Getting count of comments per post...\"\n",
      "22:05:03 | INFO | \"Comments per post summary:\n",
      "  comment_count_  posts_count  percent_of_posts  cumulative_percent_of_posts\n",
      "0            0.0      7974705          0.944907                     0.944907\n",
      "1            1.0        52065          0.006169                     0.951076\n",
      "2            2.0        55393          0.006563                     0.957639\n",
      "3            3.0        48544          0.005752                     0.963391\n",
      "4             4+       308965          0.036609                     1.000000\"\n",
      "22:05:11 | INFO | \"    464,967 <- Posts that need weighted average\"\n",
      "22:05:13 | WARNING | \"DEBUGGING: get df_posts_for_weights, SHAPE:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929934, 516)\n",
      "First columns:\n",
      "  ['subreddit_name', 'subreddit_id', 'post_id', 'embeddings_0', 'embeddings_1', 'embeddings_2', 'embeddings_3', 'embeddings_4', 'embeddings_5', 'embeddings_6']\n",
      "Last columns:\n",
      "  ['embeddings_503', 'embeddings_504', 'embeddings_505', 'embeddings_506', 'embeddings_507', 'embeddings_508', 'embeddings_509', 'embeddings_510', 'embeddings_511', '_col_method_weight_']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 464967/464967 [09:21<00:00, 827.91it/s]\n",
      "22:14:59 | INFO | \"  (464967, 512) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'post_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings_pd.py\u001b[0m in \u001b[0;36mrun_aggregation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Weights by inputs, e.g., 70% post, 20% comments, 10% subreddit description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_posts_and_comments_to_post_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_ram_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_memory_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_posts_comments_and_sub_descriptions_to_post_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings_pd.py\u001b[0m in \u001b[0;36m_agg_posts_and_comments_to_post_level\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;31m# Re-append multi-index so it's the same in original and new output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         df_agg_posts_w_comments = (\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0mdf_agg_posts_w_comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             .merge(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   8206\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8207\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8208\u001b[0;31m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8209\u001b[0m         )\n\u001b[1;32m   8210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'post_id'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "try:\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    job_agg_test._send_log_file_to_mlflow()\n",
    "    setup_logging()\n",
    "    del job_agg_test\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "mlflow.end_run(\"FAILED\")\n",
    "\n",
    "job_agg_test = aggregate_embeddings_pd.AggregateEmbeddings(\n",
    "    run_name=f\"sample_test_lc_false_pd-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    \n",
    "    # use pre-loaded dfs\n",
    "    df_v_posts=df_v_posts_test_sample,\n",
    "    df_v_sub=df_v_sub_test_sample,\n",
    "    df_v_comments=df_v_comments_test_sample,\n",
    "    df_posts_meta=df_posts_meta_,\n",
    "    df_comments_meta=df_comments_meta_,\n",
    "    \n",
    "    **config_test_sample_lc_false.config_flat\n",
    ")\n",
    "job_agg_test.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca878673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:55 | INFO | \"Logging log-file to mlflow...\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4462"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_agg_test._send_log_file_to_mlflow()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c791344",
   "metadata": {},
   "source": [
    "### Create pre-loaded dfs to save on loading time\n",
    "\n",
    "After we know loading works, this could save 3-4 minutes per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e93f259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8439672, 515)\n",
      "(19262, 514)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_v_posts_test_sample = job_agg_test.df_v_posts.copy()\n",
    "print(df_v_posts_test_sample.shape)\n",
    "\n",
    "df_v_sub_test_sample = job_agg_test.df_v_sub.copy()\n",
    "print(df_v_sub_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfd747df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2649171, 516)\n",
      "CPU times: user 1.15 s, sys: 1.42 s, total: 2.57 s\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_v_comments_test_sample = job_agg_test.df_v_comments.copy()\n",
    "print(df_v_comments_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b048b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 7 s, total: 24 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_posts_meta_ = job_agg_test.df_posts_meta\n",
    "df_comments_meta_ = job_agg_test.df_comments_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1238774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8553535f",
   "metadata": {},
   "source": [
    "### Check computed dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94434026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_v_posts\n",
      "  (8439672, 515)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_oy5757</td>\n",
       "      <td>-0.024560</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>-0.030832</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>-0.069964</td>\n",
       "      <td>0.058501</td>\n",
       "      <td>0.011466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p7959y</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.028851</td>\n",
       "      <td>-0.034535</td>\n",
       "      <td>-0.101000</td>\n",
       "      <td>0.031001</td>\n",
       "      <td>0.030201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p9qjt4</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.053867</td>\n",
       "      <td>-0.044054</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>-0.112127</td>\n",
       "      <td>-0.015414</td>\n",
       "      <td>0.066434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p6pby5</td>\n",
       "      <td>0.027264</td>\n",
       "      <td>-0.019307</td>\n",
       "      <td>0.031788</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.034353</td>\n",
       "      <td>0.036209</td>\n",
       "      <td>-0.051651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p01h3v</td>\n",
       "      <td>-0.005145</td>\n",
       "      <td>0.039692</td>\n",
       "      <td>-0.043006</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>-0.092230</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p6ww7c</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>-0.021896</td>\n",
       "      <td>0.044703</td>\n",
       "      <td>-0.079627</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>-0.048790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_paf7xc</td>\n",
       "      <td>-0.020565</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>-0.084957</td>\n",
       "      <td>0.050607</td>\n",
       "      <td>0.064898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_pprskd</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>-0.018898</td>\n",
       "      <td>0.031398</td>\n",
       "      <td>0.035929</td>\n",
       "      <td>-0.072952</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.025411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_name subreddit_id    post_id  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6\n",
       "0  circumcisiongrief     t5_zzszh  t3_oy5757     -0.024560      0.010143     -0.030832      0.037089     -0.069964      0.058501      0.011466\n",
       "1  circumcisiongrief     t5_zzszh  t3_p7959y     -0.024816     -0.002123     -0.028851     -0.034535     -0.101000      0.031001      0.030201\n",
       "2  circumcisiongrief     t5_zzszh  t3_p9qjt4      0.003813      0.053867     -0.044054      0.007976     -0.112127     -0.015414      0.066434\n",
       "3  circumcisiongrief     t5_zzszh  t3_p6pby5      0.027264     -0.019307      0.031788      0.006627      0.034353      0.036209     -0.051651\n",
       "4  circumcisiongrief     t5_zzszh  t3_p01h3v     -0.005145      0.039692     -0.043006      0.018923     -0.092230     -0.000506      0.005156\n",
       "5  circumcisiongrief     t5_zzszh  t3_p6ww7c     -0.005935      0.007493     -0.021896      0.044703     -0.079627      0.014662     -0.048790\n",
       "6  circumcisiongrief     t5_zzszh  t3_paf7xc     -0.020565      0.059700     -0.000603      0.023017     -0.084957      0.050607      0.064898\n",
       "7  circumcisiongrief     t5_zzszh  t3_pprskd      0.017955     -0.018898      0.031398      0.035929     -0.072952      0.009319      0.025411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8439672 entries, 0 to 8439671\n",
      "Columns: 515 entries, subreddit_name to embeddings_511\n",
      "dtypes: float32(512), object(3)\n",
      "memory usage: 16.3+ GB\n",
      "None\n",
      "\n",
      "df_v_comments\n",
      "  (2649171, 516)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_p90j2e</td>\n",
       "      <td>t1_h9w0eth</td>\n",
       "      <td>0.012936</td>\n",
       "      <td>-0.024696</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>-0.096880</td>\n",
       "      <td>0.075883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_p9ierl</td>\n",
       "      <td>t1_h9y9a59</td>\n",
       "      <td>-0.046967</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>-0.030745</td>\n",
       "      <td>-0.077184</td>\n",
       "      <td>0.084849</td>\n",
       "      <td>0.008081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_owhp69</td>\n",
       "      <td>t1_h7g32pv</td>\n",
       "      <td>0.022889</td>\n",
       "      <td>-0.061986</td>\n",
       "      <td>-0.083042</td>\n",
       "      <td>-0.022774</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.028299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pn8y4r</td>\n",
       "      <td>t1_hcnr1bw</td>\n",
       "      <td>-0.012224</td>\n",
       "      <td>-0.042725</td>\n",
       "      <td>-0.083593</td>\n",
       "      <td>-0.027740</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>-0.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_ozito8</td>\n",
       "      <td>t1_h81169c</td>\n",
       "      <td>-0.071418</td>\n",
       "      <td>0.033446</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.013441</td>\n",
       "      <td>-0.030473</td>\n",
       "      <td>0.079096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_ozs874</td>\n",
       "      <td>t1_h82dc69</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.038823</td>\n",
       "      <td>-0.062955</td>\n",
       "      <td>-0.097027</td>\n",
       "      <td>0.051943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pqgvlr</td>\n",
       "      <td>t1_hdaztwy</td>\n",
       "      <td>0.137612</td>\n",
       "      <td>-0.013515</td>\n",
       "      <td>0.071143</td>\n",
       "      <td>-0.021872</td>\n",
       "      <td>-0.124753</td>\n",
       "      <td>0.053046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pqh6fd</td>\n",
       "      <td>t1_hdb4z1a</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>-0.043037</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>-0.076607</td>\n",
       "      <td>0.072387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_name subreddit_id    post_id  comment_id  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5\n",
       "0   0sanitymemes    t5_2qlzfy  t3_p90j2e  t1_h9w0eth      0.012936     -0.024696      0.003495      0.004394     -0.096880      0.075883\n",
       "1   0sanitymemes    t5_2qlzfy  t3_p9ierl  t1_h9y9a59     -0.046967      0.033700     -0.030745     -0.077184      0.084849      0.008081\n",
       "2   0sanitymemes    t5_2qlzfy  t3_owhp69  t1_h7g32pv      0.022889     -0.061986     -0.083042     -0.022774      0.004993      0.028299\n",
       "3   0sanitymemes    t5_2qlzfy  t3_pn8y4r  t1_hcnr1bw     -0.012224     -0.042725     -0.083593     -0.027740      0.028860     -0.016362\n",
       "4   0sanitymemes    t5_2qlzfy  t3_ozito8  t1_h81169c     -0.071418      0.033446      0.007267     -0.013441     -0.030473      0.079096\n",
       "5   0sanitymemes    t5_2qlzfy  t3_ozs874  t1_h82dc69      0.015912      0.019512      0.038823     -0.062955     -0.097027      0.051943\n",
       "6   0sanitymemes    t5_2qlzfy  t3_pqgvlr  t1_hdaztwy      0.137612     -0.013515      0.071143     -0.021872     -0.124753      0.053046\n",
       "7   0sanitymemes    t5_2qlzfy  t3_pqh6fd  t1_hdb4z1a      0.043433      0.007597     -0.043037      0.033520     -0.076607      0.072387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2649171 entries, 0 to 653757\n",
      "Columns: 516 entries, subreddit_name to embeddings_511\n",
      "dtypes: float32(512), object(4)\n",
      "memory usage: 5.2+ GB\n",
      "None\n",
      "\n",
      "df_v_sub\n",
      "  (19262, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>askreddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>-0.042123</td>\n",
       "      <td>-0.007514</td>\n",
       "      <td>0.049629</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.043356</td>\n",
       "      <td>-0.021046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pics</td>\n",
       "      <td>t5_2qh0u</td>\n",
       "      <td>-0.051188</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.010167</td>\n",
       "      <td>0.042715</td>\n",
       "      <td>0.034471</td>\n",
       "      <td>0.045897</td>\n",
       "      <td>-0.069887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>t5_2qh33</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>-0.029943</td>\n",
       "      <td>-0.020383</td>\n",
       "      <td>-0.022284</td>\n",
       "      <td>0.076624</td>\n",
       "      <td>0.057212</td>\n",
       "      <td>0.022809</td>\n",
       "      <td>0.036846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>memes</td>\n",
       "      <td>t5_2qjpg</td>\n",
       "      <td>-0.012688</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>-0.046276</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>0.039581</td>\n",
       "      <td>0.066430</td>\n",
       "      <td>-0.068151</td>\n",
       "      <td>0.028627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>t5_2qhsa</td>\n",
       "      <td>-0.010259</td>\n",
       "      <td>0.077889</td>\n",
       "      <td>-0.066735</td>\n",
       "      <td>0.031045</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>0.050287</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>0.007021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>holup</td>\n",
       "      <td>t5_qir9n</td>\n",
       "      <td>-0.048384</td>\n",
       "      <td>-0.075352</td>\n",
       "      <td>-0.021186</td>\n",
       "      <td>-0.018942</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>-0.002732</td>\n",
       "      <td>0.013651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>publicfreakout</td>\n",
       "      <td>t5_2yrq6</td>\n",
       "      <td>-0.029560</td>\n",
       "      <td>0.051576</td>\n",
       "      <td>-0.032588</td>\n",
       "      <td>-0.019716</td>\n",
       "      <td>0.072152</td>\n",
       "      <td>0.047094</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>-0.041286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>facepalm</td>\n",
       "      <td>t5_2r5rp</td>\n",
       "      <td>-0.054565</td>\n",
       "      <td>-0.035727</td>\n",
       "      <td>0.053798</td>\n",
       "      <td>-0.045296</td>\n",
       "      <td>0.088955</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>-0.026451</td>\n",
       "      <td>0.055427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subreddit_name subreddit_id  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7\n",
       "__null_dask_index__                                                                                                                                                \n",
       "0                            askreddit     t5_2qh1i      0.022812     -0.042123     -0.007514      0.049629      0.076650      0.037742      0.043356     -0.021046\n",
       "1                                 pics     t5_2qh0u     -0.051188      0.001655      0.036857      0.010167      0.042715      0.034471      0.045897     -0.069887\n",
       "2                                funny     t5_2qh33      0.052985     -0.029943     -0.020383     -0.022284      0.076624      0.057212      0.022809      0.036846\n",
       "3                                memes     t5_2qjpg     -0.012688      0.007123     -0.046276      0.013266      0.039581      0.066430     -0.068151      0.028627\n",
       "4                    interestingasfuck     t5_2qhsa     -0.010259      0.077889     -0.066735      0.031045      0.072704      0.050287      0.033434      0.007021\n",
       "5                                holup     t5_qir9n     -0.048384     -0.075352     -0.021186     -0.018942      0.075612      0.072915     -0.002732      0.013651\n",
       "6                       publicfreakout     t5_2yrq6     -0.029560      0.051576     -0.032588     -0.019716      0.072152      0.047094      0.004342     -0.041286\n",
       "7                             facepalm     t5_2r5rp     -0.054565     -0.035727      0.053798     -0.045296      0.088955      0.029438     -0.026451      0.055427"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19262 entries, 0 to 19261\n",
      "Columns: 514 entries, subreddit_name to embeddings_511\n",
      "dtypes: float32(512), object(2)\n",
      "memory usage: 38.1+ MB\n",
      "None\n",
      "\n",
      "df_subs_meta\n",
      "  (19262, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt_date</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>geo_relevant_country_codes</th>\n",
       "      <th>geo_relevant_countries</th>\n",
       "      <th>geo_relevant_country_count</th>\n",
       "      <th>geo_relevant_subreddit</th>\n",
       "      <th>ambassador_subreddit</th>\n",
       "      <th>combined_topic</th>\n",
       "      <th>combined_topic_and_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>pics</td>\n",
       "      <td>t5_2qh0u</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>art</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>funny</td>\n",
       "      <td>t5_2qh33</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>memes</td>\n",
       "      <td>t5_2qjpg</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>t5_2qhsa</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>holup</td>\n",
       "      <td>t5_qir9n</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>publicfreakout</td>\n",
       "      <td>t5_2yrq6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>over18_nsfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-09-21</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>t5_2r5rp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uncategorized</td>\n",
       "      <td>uncategorized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pt_date     subreddit_name subreddit_id geo_relevant_country_codes geo_relevant_countries  geo_relevant_country_count  geo_relevant_subreddit  ambassador_subreddit combined_topic combined_topic_and_rating\n",
       "0  2021-09-21          askreddit     t5_2qh1i                       None                   None                         NaN                   False                 False  uncategorized             uncategorized\n",
       "1  2021-09-21               pics     t5_2qh0u                       None                   None                         NaN                   False                 False            art                       art\n",
       "2  2021-09-21              funny     t5_2qh33                       None                   None                         NaN                   False                 False  uncategorized             uncategorized\n",
       "3  2021-09-21              memes     t5_2qjpg                       None                   None                         NaN                   False                 False  uncategorized             uncategorized\n",
       "4  2021-09-21  interestingasfuck     t5_2qhsa                       None                   None                         NaN                   False                 False  uncategorized             uncategorized\n",
       "5  2021-09-21              holup     t5_qir9n                       None                   None                         NaN                   False                 False  uncategorized             uncategorized\n",
       "6  2021-09-21     publicfreakout     t5_2yrq6                       None                   None                         NaN                   False                 False  uncategorized               over18_nsfw\n",
       "7  2021-09-21           facepalm     t5_2r5rp                       None                   None                         NaN                   False                 False  uncategorized             uncategorized"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_posts_meta\n",
      "  (8439672, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>submit_date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>combined_topic_and_rating</th>\n",
       "      <th>post_type</th>\n",
       "      <th>weighted_language</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_oy5757</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>391</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p7959y</td>\n",
       "      <td>2021-08-19</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>471</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p9qjt4</td>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>image</td>\n",
       "      <td>en</td>\n",
       "      <td>88</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p6pby5</td>\n",
       "      <td>2021-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p01h3v</td>\n",
       "      <td>2021-08-07</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>628</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_p6ww7c</td>\n",
       "      <td>2021-08-18</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>378</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_paf7xc</td>\n",
       "      <td>2021-08-24</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>136</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>t3_pprskd</td>\n",
       "      <td>2021-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>over18_nsfw</td>\n",
       "      <td>text</td>\n",
       "      <td>en</td>\n",
       "      <td>3045</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_name subreddit_id    post_id submit_date  upvotes combined_topic_and_rating post_type weighted_language  text_len  text_word_count\n",
       "0  circumcisiongrief     t5_zzszh  t3_oy5757  2021-08-04        0               over18_nsfw      text                en       391               71\n",
       "1  circumcisiongrief     t5_zzszh  t3_p7959y  2021-08-19        0               over18_nsfw      text                en       471              103\n",
       "2  circumcisiongrief     t5_zzszh  t3_p9qjt4  2021-08-23        0               over18_nsfw     image                en        88               17\n",
       "3  circumcisiongrief     t5_zzszh  t3_p6pby5  2021-08-18        0               over18_nsfw      text                en        23                3\n",
       "4  circumcisiongrief     t5_zzszh  t3_p01h3v  2021-08-07        0               over18_nsfw      text                en       628              130\n",
       "5  circumcisiongrief     t5_zzszh  t3_p6ww7c  2021-08-18        0               over18_nsfw      text                en       378               73\n",
       "6  circumcisiongrief     t5_zzszh  t3_paf7xc  2021-08-24        0               over18_nsfw      text                en       136               24\n",
       "7  circumcisiongrief     t5_zzszh  t3_pprskd  2021-09-17        0               over18_nsfw      text                en      3045              592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_comments_meta\n",
      "  (39901968, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>submit_date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comment_text_len</th>\n",
       "      <th>comment_text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_p90j2e</td>\n",
       "      <td>t1_h9w0eth</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>14</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_p9ierl</td>\n",
       "      <td>t1_h9y9a59</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>31</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_owhp69</td>\n",
       "      <td>t1_h7g32pv</td>\n",
       "      <td>2021-08-02</td>\n",
       "      <td>95</td>\n",
       "      <td>102</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pn8y4r</td>\n",
       "      <td>t1_hcnr1bw</td>\n",
       "      <td>2021-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>948</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_ozito8</td>\n",
       "      <td>t1_h81169c</td>\n",
       "      <td>2021-08-07</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_ozs874</td>\n",
       "      <td>t1_h82dc69</td>\n",
       "      <td>2021-08-07</td>\n",
       "      <td>11</td>\n",
       "      <td>129</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pqgvlr</td>\n",
       "      <td>t1_hdaztwy</td>\n",
       "      <td>2021-09-18</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0sanitymemes</td>\n",
       "      <td>t5_2qlzfy</td>\n",
       "      <td>t3_pqh6fd</td>\n",
       "      <td>t1_hdb4z1a</td>\n",
       "      <td>2021-09-18</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_name subreddit_id    post_id  comment_id submit_date  upvotes  comment_text_len  comment_text_word_count\n",
       "0   0sanitymemes    t5_2qlzfy  t3_p90j2e  t1_h9w0eth  2021-08-22       14               144                       34\n",
       "1   0sanitymemes    t5_2qlzfy  t3_p9ierl  t1_h9y9a59  2021-08-22       31                69                       12\n",
       "2   0sanitymemes    t5_2qlzfy  t3_owhp69  t1_h7g32pv  2021-08-02       95               102                       20\n",
       "3   0sanitymemes    t5_2qlzfy  t3_pn8y4r  t1_hcnr1bw  2021-09-13        0               948                      135\n",
       "4   0sanitymemes    t5_2qlzfy  t3_ozito8  t1_h81169c  2021-08-07        5                82                       14\n",
       "5   0sanitymemes    t5_2qlzfy  t3_ozs874  t1_h82dc69  2021-08-07       11               129                       26\n",
       "6   0sanitymemes    t5_2qlzfy  t3_pqgvlr  t1_hdaztwy  2021-09-18        7                 9                        3\n",
       "7   0sanitymemes    t5_2qlzfy  t3_pqh6fd  t1_hdb4z1a  2021-09-18        8                90                       22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_comment_count_per_post\n",
      "  (8439672, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>comment_count_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_ovhuwk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_ovhuza</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_ovhuzm</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_ovhv4w</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_ovhv7e</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_ovhvak</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_ovhvba</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_ovhvfr</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     post_id  comment_count comment_count_\n",
       "0  t3_ovhuwk            1.0            1.0\n",
       "1  t3_ovhuza            9.0             4+\n",
       "2  t3_ovhuzm            7.0             4+\n",
       "3  t3_ovhv4w           14.0             4+\n",
       "4  t3_ovhv7e            1.0            1.0\n",
       "5  t3_ovhvak            3.0            3.0\n",
       "6  t3_ovhvba            4.0             4+\n",
       "7  t3_ovhvfr            9.0             4+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8439672 entries, 0 to 8439671\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   post_id         object \n",
      " 1   comment_count   float64\n",
      " 2   comment_count_  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 257.6+ MB\n",
      "None\n",
      "\n",
      "df_posts_agg_b\n",
      "\n",
      "df_posts_agg_c\n",
      "\n",
      "df_subs_agg_a\n",
      "\n",
      "df_subs_agg_b\n",
      "\n",
      "df_subs_agg_c\n",
      "\n",
      "df_subs_agg_a_similarity\n",
      "\n",
      "df_subs_agg_b_similarity\n",
      "\n",
      "df_subs_agg_c_similarity\n",
      "\n",
      "df_subs_agg_a_similarity_pair\n",
      "\n",
      "df_subs_agg_b_similarity_pair\n",
      "\n",
      "df_subs_agg_c_similarity_pair\n",
      "\n",
      "df_v_com_agg\n",
      "  (464967, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">0sanitymemes</th>\n",
       "      <th>t3_ovly4k</th>\n",
       "      <td>-0.001755</td>\n",
       "      <td>-0.014411</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.037531</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>-0.015870</td>\n",
       "      <td>-0.070303</td>\n",
       "      <td>0.006013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovmkrd</th>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.035019</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>-0.032112</td>\n",
       "      <td>0.069218</td>\n",
       "      <td>-0.003567</td>\n",
       "      <td>-0.027806</td>\n",
       "      <td>-0.080121</td>\n",
       "      <td>0.004848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovnz2q</th>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.015113</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>-0.039074</td>\n",
       "      <td>0.039467</td>\n",
       "      <td>0.048250</td>\n",
       "      <td>-0.008903</td>\n",
       "      <td>-0.105664</td>\n",
       "      <td>-0.028115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovp369</th>\n",
       "      <td>0.054907</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.069496</td>\n",
       "      <td>0.041878</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.070194</td>\n",
       "      <td>-0.077451</td>\n",
       "      <td>-0.028698</td>\n",
       "      <td>-0.004772</td>\n",
       "      <td>-0.047805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovpexl</th>\n",
       "      <td>0.129815</td>\n",
       "      <td>-0.022494</td>\n",
       "      <td>0.036611</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>-0.079794</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.036529</td>\n",
       "      <td>-0.012339</td>\n",
       "      <td>-0.064828</td>\n",
       "      <td>-0.007183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovqia6</th>\n",
       "      <td>-0.002432</td>\n",
       "      <td>-0.025784</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>-0.011705</td>\n",
       "      <td>0.020703</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>-0.003458</td>\n",
       "      <td>-0.062023</td>\n",
       "      <td>0.026739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovubi1</th>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>-0.015722</td>\n",
       "      <td>0.029091</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>-0.003224</td>\n",
       "      <td>-0.076562</td>\n",
       "      <td>0.015392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_ovuy09</th>\n",
       "      <td>0.032474</td>\n",
       "      <td>-0.008697</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>-0.007205</td>\n",
       "      <td>-0.050312</td>\n",
       "      <td>0.024080</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>-0.084380</td>\n",
       "      <td>-0.001551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name post_id                                                                                                                                              \n",
       "0sanitymemes   t3_ovly4k     -0.001755     -0.014411      0.031800      0.001120     -0.037531      0.020433      0.008061     -0.015870     -0.070303      0.006013\n",
       "               t3_ovmkrd     -0.015998     -0.035019      0.025448      0.008916     -0.032112      0.069218     -0.003567     -0.027806     -0.080121      0.004848\n",
       "               t3_ovnz2q      0.010460      0.009205      0.015113      0.027652     -0.039074      0.039467      0.048250     -0.008903     -0.105664     -0.028115\n",
       "               t3_ovp369      0.054907      0.007499      0.069496      0.041878      0.001678      0.070194     -0.077451     -0.028698     -0.004772     -0.047805\n",
       "               t3_ovpexl      0.129815     -0.022494      0.036611      0.015707     -0.079794      0.056509      0.036529     -0.012339     -0.064828     -0.007183\n",
       "               t3_ovqia6     -0.002432     -0.025784      0.036367      0.001857     -0.011705      0.020703      0.006595     -0.003458     -0.062023      0.026739\n",
       "               t3_ovubi1      0.004887      0.010785     -0.000012     -0.002698     -0.015722      0.029091      0.000425     -0.003224     -0.076562      0.015392\n",
       "               t3_ovuy09      0.032474     -0.008697      0.004628     -0.007205     -0.050312      0.024080      0.002667      0.009686     -0.084380     -0.001551"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 464967 entries, ('0sanitymemes', 't3_ovly4k') to ('geartrade', 't3_psu4a9')\n",
      "Columns: 512 entries, embeddings_0 to embeddings_511\n",
      "dtypes: float32(512)\n",
      "memory usage: 914.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for k_, v_ in {k_: v_ for k_, v_ in vars(job_agg_test).items() if 'df_' in k_}.items():\n",
    "    print(f\"\\n{k_}\")\n",
    "    try:\n",
    "        print(f\"  {v_.shape}\")\n",
    "        display(v_.iloc[:8, :10])\n",
    "        if not ('meta' in k_):\n",
    "            print(v_.info())\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499e348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68612b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_agg_test._save_and_log_aggregate_and_similarity_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19e9d3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2794"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475456b",
   "metadata": {},
   "source": [
    "# Check output dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39c8e8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vars(job_agg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ed34797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_dfs2 = {k: v for k, v in vars(job_agg_test).items() if 'df_' in k}\n",
    "\n",
    "\n",
    "# for k2, df_2 in tqdm(d_dfs2.items()):\n",
    "#     print(f\"\\n{k2}\")\n",
    "#     try:\n",
    "#         print(f\"  {df_2.shape} <- df shape\")\n",
    "#         print(f\"  {df_2.npartitions} <- dask partitions\")\n",
    "#         # print(f\"{get_dask_df_shape(df_2)} <- df.shape\")\n",
    "#         # print(f\"  {df_2.memory_usage(deep=True).sum() / 1048576:4,.1f} MB <- Memory usage\")\n",
    "#         if any(['meta' in k2, '_v_' in k2]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             pass\n",
    "# #             display(df_2.iloc[:5, :15])\n",
    "\n",
    "#     except (TypeError, AttributeError):\n",
    "#         if isinstance(df_2, pd.DataFrame):\n",
    "#             print(f\"  {df_2.shape} <- df shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0859c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b336399b",
   "metadata": {},
   "source": [
    "## VM size notes\n",
    "\n",
    "`614 GB` of RAM is not enough for 40 million posts...\n",
    "\n",
    "VM & cluster set up:\n",
    "```bash\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```\n",
    "\n",
    "Traceback:\n",
    "\n",
    "```bash\n",
    "23:17:48 | INFO | \"      775,092 <- Comments that DON'T need to be averaged\"\n",
    "23:17:48 | INFO | \"   39,126,876 <- Comments that need to be averaged\"\n",
    "23:17:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "---------------------------------------------------------------------------\n",
    "KilledWorker                              Traceback (most recent call last)\n",
    "<timed exec> in <module>\n",
    "\n",
    "/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py in run_aggregation(self)\n",
    "    249         # - up-votes\n",
    "    250         # ---\n",
    "--> 251         self._agg_comments_to_post_level()\n",
    "...\n",
    "KilledWorker: (\"('dataframe-groupby-sum-agg-849e94fd54a49f8ed34330862f20cb9d', 0)\", <WorkerState 'tcp://127.0.0.1:41351', name: 6, memory: 0, processing: 1>)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14b214",
   "metadata": {},
   "source": [
    "### time profiling\n",
    "\n",
    "inputs:\n",
    "``` python\n",
    "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
    "n_sample_posts_files: \t5\n",
    "n_sample_comments_files: \t10\n",
    "\n",
    "aggregate_params:\n",
    "  min_comment_text_len: \t10\n",
    "  agg_comments_to_post_weight_col: \tNone\n",
    "  agg_post_to_subreddit_weight_col: \tNone\n",
    "  agg_post_post_weight: \t70\n",
    "  agg_post_comment_weight: \t20\n",
    "  agg_post_subreddit_desc_weight: \t10\n",
    "```\n",
    "\n",
    "VM & cluster set up:\n",
    "```\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a4e06",
   "metadata": {},
   "source": [
    "### Filtered/selected logs\n",
    "\n",
    "Overview:\n",
    "\n",
    "| Time/ETA | Step | Notes |\n",
    "| --- | --- | --- |\n",
    "| `0:11:43` minutes | load raw embeddings (w/o caching) | |\n",
    "| `0:03:15` minutes | Load metadata (w/o caching):  |  | \n",
    "| `0:04:30` minutes | Aggegation steps (all) | Note that this might only be the time to create the dag, not necessarily the time to actually compute the data | \n",
    "| `0:37:24` minutes | Calculate similarities  |  | \n",
    "| `1:30:00` HOURS | Saving & logging files | Saving alone could take more than 1 hour... mand I'd forgotten about this | \n",
    "|  |  |  | \n",
    "\n",
    "\n",
    "Note that there's very different ETAs for saving each DF, the first 2 are really large and take a long time. The last few are smaller, so the time estimates from `tqdm` can vary a ton:\n",
    "```bash\n",
    "3/11 [40:25<1:14:34, 559.33s/it]   27%\n",
    "9/11 [50:22<03:22, 101.02s/it]     82% \n",
    "11/11 [1:30:11<00:00, 750.74s/it] 100%\n",
    "```\n",
    "\n",
    "\n",
    "Getting shape of `dask df` is taking almost half of the saving time!\n",
    "\n",
    "**TODO: REMOVE** logging df shape for now to save a ton of time!\n",
    "\n",
    "```bash\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"                                   # get_df_shape() starts here...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"             # get df_shape() ends here, ABOUT 40 MINUTES!\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"                       # In contrast, SAVING the dask df only takes about 20 MINUTES!\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"    # And logging the dfs up to GCS only takes about 3 seconds?!\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "21:33:14 | INFO | \"Keeping index intact...\"\n",
    "21:33:14 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:15 | INFO | \"   185.4 MB <- Memory usage\"\n",
    "21:33:15 | INFO | \"       5\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
    "21:33:19 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:22 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
    "21:33:22 | INFO | \"Saving locally...\"\n",
    "21:33:24 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:35 | INFO | \"  10,391.8 MB <- Memory usage\"\n",
    "21:33:35 | INFO | \"     139\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
    "21:33:55 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:34:30 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593704f",
   "metadata": {},
   "source": [
    "More details in log file:\n",
    "\n",
    "`logs/AggregateEmbeddings/2021-10-05_195710-sample_test_lc_false-2021-10-05_195710.log`\n",
    "\n",
    "\n",
    "```bash\n",
    "# load raw embeddings (w/o caching): 11:43 minutes\n",
    "# ---\n",
    "20:01:19 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
    "40/40 [07:29<00:00, 8.17s/it]\n",
    "20:08:49 | INFO | \"  Parquet files found: 5\"\n",
    "20:08:49 | INFO | \"  Keep only comments for posts with embeddings\"\n",
    "20:08:54 | INFO | \"  0:11:43.326935 <- Total raw embeddings load time elapsed\"\n",
    "\n",
    "\n",
    "# Load metadata (w/o caching): 3:15 minutes\n",
    "# ---\n",
    "20:08:54 | INFO | \"-- Start _load_metadata() method --\"\n",
    "20:08:54 | INFO | \"Loading POSTS metadata...\"\n",
    "\n",
    "20:10:39 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
    "100%\n",
    "59/59 [01:30<00:00, 1.43s/it]\n",
    "20:12:10 | INFO | \"  (Delayed('int-e6188e6d-6319-487d-b054-bea8a30d912b'), 7) <- Raw META COMMENTS shape\"\n",
    "20:12:10 | INFO | \"  0:03:15.218880 <- Total metadata loading time elapsed\"\n",
    "\n",
    "\n",
    "# Aggegation steps (all): 4:30 minutes\n",
    "#   Note that this might only be the time to create the dag, not necessarily the time to actually compute the data\n",
    "20:12:10 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
    "20:12:10 | INFO | \"Getting count of comments per post...\"\n",
    "20:12:39 | INFO | \"Filtering which comments need to be averaged...\"\n",
    "20:13:23 | INFO | \"       22,197 <- Comments that DON'T need to be averaged\"\n",
    "20:13:23 | INFO | \"    1,087,458 <- Comments that need to be averaged\"\n",
    "20:13:28 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "20:13:57 | INFO | \"      191,558 |  514 <- df_v_com_agg SHAPE\"\n",
    "20:13:57 | INFO | \"  0:01:46.878385 <- Total comments to post agg loading time elapsed\"\n",
    "20:13:57 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
    "20:13:59 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
    "...\n",
    "20:16:38 | INFO | \"A - posts only\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-3ee084d4-434c-4a38-aeb1-185b50648908'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
    "20:16:39 | INFO | \"B - posts + comments\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-6bd21f72-fc1d-495c-987a-1da6e4a18683'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
    "20:16:39 | INFO | \"C - posts + comments + sub descriptions\"\n",
    "20:16:40 | INFO | \"  (Delayed('int-59c54047-7ac8-49cb-81b3-478b4ae5b60e'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
    "20:16:40 | INFO | \"  0:00:01.507065 <- Total for ALL subreddit-level agg time elapsed\"\n",
    "\n",
    "\n",
    "# Calculate similarities 37:24 minutes\n",
    "20:16:40 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
    "20:16:40 | INFO | \"A...\"\n",
    "20:16:56 | INFO | \"  (4924, 4924) <- df_subs_agg_a_similarity.shape\"\n",
    "20:17:21 | INFO | \"Merge distance + metadata...\"\n",
    "20:17:59 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
    "20:18:10 | INFO | \"  (24240852, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
    "20:18:10 | INFO | \"  (98480, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
    "...\n",
    "20:54:04 | INFO | \"  0:37:24.347689 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
    "\n",
    "\n",
    "# *** Saving & logging file: WTF? Saving alone could take more than 2 hours!! WTF?!!  ***\n",
    "20:54:04 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
    "20:54:04 | INFO | \"  Saving config to local path...\"\n",
    "20:54:04 | INFO | \"  Logging config to mlflow...\"\n",
    "*** 3/11 [40:25<1:14:34, 559.33s/it]  27%   ***\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"\n",
    "...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886f38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
