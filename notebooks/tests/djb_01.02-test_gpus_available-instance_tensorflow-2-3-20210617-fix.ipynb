{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1e2722",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Check whether GPUs are available/usable in current python environment.\n",
    "\n",
    "\n",
    "# Debugging notes\n",
    "## Make sure to set the correct venv/kernel in your notebook\n",
    "The default `python 3` might not have the correct drivers.\n",
    "<br>Instead, might need to manually set it to:\n",
    "<br>`Python [conda env:root]`\n",
    "\n",
    "## Sometimes the best fix is `sudo reboot`\n",
    "When in doubt, open a terminal and do `sudo reboot`.\n",
    "\n",
    "For some reason, the NVIDIA drivers might not be loaded properly after shutting down a VM instance from the GUI:\n",
    "- https://console.cloud.google.com/ai-platform/notebooks/list/instances?project=data-prod-165221\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff33e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ca318",
   "metadata": {},
   "source": [
    "# Imports & notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561629d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1b61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "tensorflow\tv: 2.3.2\n",
      "tensorflow_text\tv: 2.3.0\n",
      "subclu\t\tv: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, notebook_display_config, print_lib_versions,\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([tf, tensorflow_text, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc74145",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2a4fd",
   "metadata": {},
   "source": [
    "# Check GPUs recognized by Tensorflow/python\n",
    "\n",
    "NOTE: `GPU`s and `XLA_GPU`s are recognized as two different device types.\n",
    "\n",
    "https://www.tensorflow.org/xla\n",
    "> **XLA: Optimizing Compiler for Machine Learning**\n",
    "> XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n",
    "> \n",
    "> The results are improvements in speed and memory usage: e.g. in BERT MLPerf submission using 8 Volta V100 GPUs using XLA has achieved a ~7x performance improvement and ~5x batch size improvement\n",
    "\n",
    "Other sources\n",
    "- https://stackoverflow.com/questions/52943489/what-is-xla-gpu-and-xla-cpu-for-tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a060c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eccbbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "CPU times: user 1.39 ms, sys: 1.23 ms, total: 2.61 ms\n",
      "Wall time: 1.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b550c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 0\n",
      "GPU details:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = (\n",
    "    tf.config.list_physical_devices('GPU') +\n",
    "    tf.config.list_physical_devices('XLA_GPU')\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\n\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a28436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "\n",
      "All devices:\n",
      "===\n",
      "Num devices: 2\n",
      "Details:\n",
      "[   name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4153781128868626988\n",
      ",\n",
      "    name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1204199666303421190\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "l_all_local_devices = device_lib.list_local_devices()\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\n\\nAll devices:\\n===\"\n",
    "    f\"\\nNum devices: {len(l_all_local_devices)}\"\n",
    "    f\"\\nDetails:\"\n",
    ")\n",
    "pprint(l_all_local_devices, indent=4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb98376",
   "metadata": {},
   "source": [
    "# Check NVIDIA CLI\n",
    "\n",
    "First, do we even see the GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15960ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:04.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\n",
      "00:05.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\n"
     ]
    }
   ],
   "source": [
    "!lspci | grep 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1046a3",
   "metadata": {},
   "source": [
    "Then, are they recognized by the nvidia-smi tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c6c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 30 04:11:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   72C    P0    31W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   70C    P0    22W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23147fa",
   "metadata": {},
   "source": [
    "## Debug nvidia drivers\n",
    "\n",
    "If `nvidia-smi` doesn't detect the drivers, we might need to reinstall them.\n",
    "\n",
    "- https://towardsdatascience.com/troubleshooting-gcp-cuda-nvidia-docker-and-keeping-it-running-d5c8b34b6a4c\n",
    "\n",
    "Getting nothing from `cuda` is bad... sigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4acf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvidia-container-tools             1.4.0-1                       amd64        NVIDIA container runtime library (command-line tools)\n",
      "ii  libnvidia-container1:amd64            1.4.0-1                       amd64        NVIDIA container runtime library\n",
      "ii  nvidia-container-runtime              3.5.0-1                       amd64        NVIDIA container runtime\n",
      "ii  nvidia-container-toolkit              1.5.1-1                       amd64        NVIDIA container runtime hook\n",
      "ii  nvidia-docker2                        2.6.0-1                       all          nvidia-docker CLI wrapper\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aede3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dpkg -l | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e5cd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmesg: read kernel buffer failed: Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "!dmesg | grep NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6eb30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting... Done\n",
      "Full Text Search... Done\n"
     ]
    }
   ],
   "source": [
    "!apt search nvidia-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed74647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
