{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3772230d",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading fastText embeddings & create a uSIF model\n",
    "- load post & comment text\n",
    "- train a uSIF model\n",
    "- convert the text into embeddings (at post or comment level)\n",
    "\n",
    "In this notebook I focus on using the METADATA/descriptions for each subreddit.\n",
    "\n",
    "Hypothesis: if we can get some meaningful vectors from the subreddit descriptions, we might be able to add them as an input we can use for:\n",
    "- subreddit embeddings (e.g., 10% subreddit meta + 80% post title+text + 10% comments)\n",
    "- post-level embeddings (e.g., 10% subreddit meta + 90% post title+text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263df27",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07715db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56d624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "from functools import partial\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    "    D_MODELS_CPU,\n",
    "    process_text_for_fse,\n",
    "    vectorize_text_with_fse,\n",
    ")\n",
    "from subclu.models.preprocess_text import TextPreprocessor, transform_and_tokenize_text\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12152db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd106c",
   "metadata": {},
   "source": [
    "# Set sqlite database as MLflow URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab5aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b658ef06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/mlflow/mlruns.db'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04605c10",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2bca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c42340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_id': '0',\n",
       "  'name': 'Default',\n",
       "  'artifact_location': './mlruns/0',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '1',\n",
       "  'name': 'fse_v1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/1',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '2',\n",
       "  'name': 'fse_vectorize_v1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/2',\n",
       "  'lifecycle_stage': 'active'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba77a21",
   "metadata": {},
   "source": [
    "# Inspect data for subreddit meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738eb313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 408 ms, sys: 96.3 ms, total: 505 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bucket_ = 'i18n-subreddit-clustering'\n",
    "subs_path = 'subreddits/2021-06-01'\n",
    "\n",
    "df_subs = pd.read_parquet(\n",
    "    path=f\"gs://{bucket_}/{subs_path}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf9533b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 34)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bbb9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d56453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    196.000000\n",
       "mean      88.826531\n",
       "std      115.498609\n",
       "min        3.000000\n",
       "25%       16.750000\n",
       "50%       37.000000\n",
       "75%      105.250000\n",
       "max      636.000000\n",
       "Name: subreddit_name_title_and_clean_descriptions_word_count, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs['subreddit_name_title_and_clean_descriptions_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0519c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subs[df_subs['subreddit_name_title_and_clean_descriptions_word_count'] < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbc839",
   "metadata": {},
   "source": [
    "# Call function to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a36dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, df_posts, d_ix_to_id\n",
    "gc.collect()\n",
    "\n",
    "subs_path = 'subreddits/2021-06-01'\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_subs, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    mlflow_experiment='subreddit_description_v1',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    subreddits_path=subs_path,\n",
    "    posts_path=None,\n",
    "    comments_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c911c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:32:13 | INFO | \"Start vectorize function\"\n",
      "05:32:13 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0532\"\n",
      "05:32:13 | INFO | \"Load subreddits df...\"\n",
      "05:32:14 | INFO | \"  (196, 4) <- df_comments shape\"\n",
      "05:32:14 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "05:32:14 | INFO | \"Filtering posts for SIF training...\"\n",
      "05:32:14 | INFO | \"     0 <- Exclude posts because of: subreddits filter\"\n",
      "05:32:14 | INFO | \"     0 <- Exclude posts because of: duplicated posts\"\n",
      "05:32:14 | INFO | \"     4 <- Exclude posts because of: minimum word count\"\n",
      "05:32:14 | INFO | \"   192 <- df_subs for training\"\n",
      "05:32:14 | INFO | \"Converting df_train to fse format...\"\n",
      "05:32:14 | INFO | \"  0:00:00.000328 <- Converting to fse time elapsed\"\n",
      "05:32:14 | INFO | \"Logging training df to mlflow...\"\n",
      "05:32:14 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "05:32:14 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "05:32:14 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "05:33:44 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "05:33:44 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "05:33:46 | INFO | \"  0:01:32.062999 <- Load FSE model time elapsed\"\n",
      "05:33:46 | INFO | \"Start training fse model...\"\n",
      "05:33:52 | INFO | \"Running inference on all subreddits meta...\"\n",
      "05:33:52 | INFO | \"Convert vectors to df...\"\n",
      "05:33:52 | INFO | \"(196, 300) <- Raw vectorized text shape\"\n",
      "05:33:52 | INFO | \"  0:00:00.080252 <- Raw vectorize to df only time elapsed\"\n",
      "05:33:52 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "05:33:52 | INFO | \"  Setting col_id as index...\"\n",
      "05:33:52 | INFO | \"  0:00:00.088923 <- Converting vectors to df time elapsed\"\n",
      "05:33:52 | INFO | \"  0:01:38.424600 <- Inference time time elapsed\"\n",
      "05:33:52 | INFO | \"Saving inference for subreddits description df\"\n",
      "05:33:53 | INFO | \"  Saving inference complete\"\n",
      "05:33:53 | INFO | \"  0:01:39.417750 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "# del model, df_posts, d_ix_to_id\n",
    "gc.collect()\n",
    "\n",
    "subs_path = 'subreddits/2021-06-01'\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_subs, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    mlflow_experiment='subreddit_description_v1',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=4,\n",
    "    subreddits_path=subs_path,\n",
    "    posts_path=None,\n",
    "    comments_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06d381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:35:31 | INFO | \"Start vectorize function\"\n",
      "05:35:31 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0535\"\n",
      "05:35:31 | INFO | \"Load subreddits df...\"\n",
      "05:35:31 | INFO | \"  (196, 4) <- df_comments shape\"\n",
      "05:35:31 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "05:35:32 | INFO | \"Filtering posts for SIF training...\"\n",
      "05:35:32 | INFO | \"     0 <- Exclude posts because of: subreddits filter\"\n",
      "05:35:32 | INFO | \"     0 <- Exclude posts because of: duplicated posts\"\n",
      "05:35:32 | INFO | \"     4 <- Exclude posts because of: minimum word count\"\n",
      "05:35:32 | INFO | \"   192 <- df_subs for training\"\n",
      "05:35:32 | INFO | \"Converting df_train to fse format...\"\n",
      "05:35:32 | INFO | \"  0:00:00.000334 <- Converting to fse time elapsed\"\n",
      "05:35:32 | INFO | \"Logging training df to mlflow...\"\n",
      "05:35:32 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "05:35:32 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "05:35:32 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "05:36:59 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "05:36:59 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "05:37:02 | INFO | \"  0:01:29.738103 <- Load FSE model time elapsed\"\n",
      "05:37:02 | INFO | \"Start training fse model...\"\n",
      "05:37:08 | INFO | \"Running inference on all subreddits meta...\"\n",
      "05:37:08 | INFO | \"Convert vectors to df...\"\n",
      "05:37:08 | INFO | \"(196, 300) <- Raw vectorized text shape\"\n",
      "05:37:08 | INFO | \"  0:00:00.072135 <- Raw vectorize to df only time elapsed\"\n",
      "05:37:08 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "05:37:08 | INFO | \"  Setting col_id as index...\"\n",
      "05:37:08 | INFO | \"  0:00:00.081928 <- Converting vectors to df time elapsed\"\n",
      "05:37:08 | INFO | \"  0:01:36.291080 <- Inference time time elapsed\"\n",
      "05:37:08 | INFO | \"Saving inference for subreddits description df\"\n",
      "05:37:08 | INFO | \"  Saving inference complete\"\n",
      "05:37:08 | INFO | \"  0:01:37.322073 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_subs, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment='subreddit_description_v1',\n",
    "    tokenize_function='sklearn',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    subreddits_path=subs_path,\n",
    "    posts_path=None,\n",
    "    comments_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54e393af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:47:53 | INFO | \"Start vectorize function\"\n",
      "05:47:53 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0547\"\n",
      "05:47:53 | INFO | \"Load subreddits df...\"\n",
      "05:47:53 | INFO | \"  (196, 4) <- df_comments shape\"\n",
      "05:47:53 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "05:47:54 | INFO | \"Filtering posts for SIF training...\"\n",
      "05:47:54 | INFO | \"     0 <- Exclude posts because of: subreddits filter\"\n",
      "05:47:54 | INFO | \"     0 <- Exclude posts because of: duplicated posts\"\n",
      "05:47:54 | INFO | \"    13 <- Exclude posts because of: minimum word count\"\n",
      "05:47:54 | INFO | \"   183 <- df_subs for training\"\n",
      "05:47:54 | INFO | \"Converting df_train to fse format...\"\n",
      "05:47:54 | INFO | \"  0:00:00.000312 <- Converting to fse time elapsed\"\n",
      "05:47:54 | INFO | \"Logging training df to mlflow...\"\n",
      "05:47:54 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "05:47:54 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "05:47:54 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "05:49:21 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "05:49:21 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "05:49:24 | INFO | \"  0:01:29.954221 <- Load FSE model time elapsed\"\n",
      "05:49:24 | INFO | \"Start training fse model...\"\n",
      "05:49:30 | INFO | \"Running inference on all subreddits meta...\"\n",
      "05:49:30 | INFO | \"Convert vectors to df...\"\n",
      "05:49:30 | INFO | \"(196, 300) <- Raw vectorized text shape\"\n",
      "05:49:30 | INFO | \"  0:00:00.079694 <- Raw vectorize to df only time elapsed\"\n",
      "05:49:30 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "05:49:30 | INFO | \"  Setting col_id as index...\"\n",
      "05:49:30 | INFO | \"  0:00:00.088330 <- Converting vectors to df time elapsed\"\n",
      "05:49:30 | INFO | \"  0:00:00.089439 <- Subreddits description inference time time elapsed\"\n",
      "05:49:30 | INFO | \"Saving inference for subreddits description df\"\n",
      "05:49:30 | INFO | \"  Saving inference complete\"\n",
      "05:49:30 | INFO | \"  0:01:37.394716 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_subs, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment='subreddit_description_v1',\n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=7,\n",
    "    subreddits_path=subs_path,\n",
    "    posts_path=None,\n",
    "    comments_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6024925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:56:33 | INFO | \"Start vectorize function\"\n",
      "05:56:33 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0556\"\n",
      "05:56:33 | INFO | \"Load subreddits df...\"\n",
      "05:56:34 | INFO | \"  (196, 4) <- df_comments shape\"\n",
      "05:56:34 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "05:56:34 | INFO | \"Filtering posts for SIF training...\"\n",
      "05:56:34 | INFO | \"     0 <- Exclude posts because of: subreddits filter\"\n",
      "05:56:34 | INFO | \"     0 <- Exclude posts because of: duplicated posts\"\n",
      "05:56:34 | INFO | \"    13 <- Exclude posts because of: minimum word count\"\n",
      "05:56:34 | INFO | \"   183 <- df_subs for training\"\n",
      "05:56:34 | INFO | \"Converting df_train to fse format...\"\n",
      "05:56:34 | INFO | \"  0:00:00.000345 <- Converting to fse time elapsed\"\n",
      "05:56:34 | INFO | \"Logging training df to mlflow...\"\n",
      "05:56:34 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "05:56:34 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "05:56:34 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "05:58:08 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "05:58:08 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "05:58:11 | INFO | \"  0:01:37.425380 <- Load FSE model time elapsed\"\n",
      "05:58:11 | INFO | \"Start training fse model...\"\n",
      "05:58:19 | INFO | \"Running inference on all subreddits meta...\"\n",
      "05:58:19 | INFO | \"Convert vectors to df...\"\n",
      "05:58:19 | INFO | \"(196, 300) <- Raw vectorized text shape\"\n",
      "05:58:19 | INFO | \"  0:00:00.091942 <- Raw vectorize to df only time elapsed\"\n",
      "05:58:19 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "05:58:19 | INFO | \"  Setting col_id as index...\"\n",
      "05:58:19 | INFO | \"Merge vectors with df...\"\n",
      "05:58:19 | INFO | \"  0:00:00.124629 <- Merging df_vect with ID columns time elapsed\"\n",
      "05:58:19 | INFO | \"  0:00:00.126835 <- Converting vectors to df full time elapsed\"\n",
      "05:58:19 | INFO | \"  0:00:00.128042 <- Subreddits description inference time time elapsed\"\n",
      "05:58:19 | INFO | \"Saving inference for subreddits description df\"\n",
      "05:58:19 | INFO | \"  Saving inference complete\"\n",
      "05:58:19 | INFO | \"  0:01:46.271974 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_subs, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment='subreddit_description_v1',\n",
    "    tokenize_function='sklearn',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=7,\n",
    "    subreddits_path=subs_path,\n",
    "    posts_path=None,\n",
    "    comments_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0347c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c76c6bcd",
   "metadata": {},
   "source": [
    "# Recover artifact from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "845fd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'a6f09bcae7b147f693f6083b56ec3ad5'\n",
    "run = mlflow.get_run(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17992e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://i18n-subreddit-clustering/mlflow/mlruns/3/a6f09bcae7b147f693f6083b56ec3ad5/artifacts/d_ix_to_id/d_ix_to_id.csv'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "395c4a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_index</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>t5_22i0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>t5_30305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>t5_2s82y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>t5_37k29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>t5_2qi4z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   training_index subreddit_id\n",
       "0               0      t5_22i0\n",
       "1               1     t5_30305\n",
       "2               2     t5_2s82y\n",
       "3               3     t5_37k29\n",
       "4               4     t5_2qi4z"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx = pd.read_csv(f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\")\n",
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4d97be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.6 ms, sys: 2.19 ms, total: 83.8 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_vects = pd.read_parquet(f\"{run.info.artifact_uri}/df_vect_subreddits_description/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5bc9e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e20a113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 196 entries, ('de', 't5_22i0') to ('dotade', 't5_4d9b0q')\n",
      "Columns: 300 entries, embeddings_0 to embeddings_299\n",
      "dtypes: float32(300)\n",
      "memory usage: 241.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_vects.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f04c9913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "      <th>embeddings_10</th>\n",
       "      <th>embeddings_11</th>\n",
       "      <th>embeddings_12</th>\n",
       "      <th>embeddings_13</th>\n",
       "      <th>embeddings_14</th>\n",
       "      <th>embeddings_15</th>\n",
       "      <th>embeddings_16</th>\n",
       "      <th>embeddings_17</th>\n",
       "      <th>embeddings_18</th>\n",
       "      <th>embeddings_19</th>\n",
       "      <th>embeddings_20</th>\n",
       "      <th>embeddings_21</th>\n",
       "      <th>embeddings_22</th>\n",
       "      <th>embeddings_23</th>\n",
       "      <th>embeddings_24</th>\n",
       "      <th>embeddings_25</th>\n",
       "      <th>embeddings_26</th>\n",
       "      <th>embeddings_27</th>\n",
       "      <th>embeddings_28</th>\n",
       "      <th>embeddings_29</th>\n",
       "      <th>...</th>\n",
       "      <th>embeddings_270</th>\n",
       "      <th>embeddings_271</th>\n",
       "      <th>embeddings_272</th>\n",
       "      <th>embeddings_273</th>\n",
       "      <th>embeddings_274</th>\n",
       "      <th>embeddings_275</th>\n",
       "      <th>embeddings_276</th>\n",
       "      <th>embeddings_277</th>\n",
       "      <th>embeddings_278</th>\n",
       "      <th>embeddings_279</th>\n",
       "      <th>embeddings_280</th>\n",
       "      <th>embeddings_281</th>\n",
       "      <th>embeddings_282</th>\n",
       "      <th>embeddings_283</th>\n",
       "      <th>embeddings_284</th>\n",
       "      <th>embeddings_285</th>\n",
       "      <th>embeddings_286</th>\n",
       "      <th>embeddings_287</th>\n",
       "      <th>embeddings_288</th>\n",
       "      <th>embeddings_289</th>\n",
       "      <th>embeddings_290</th>\n",
       "      <th>embeddings_291</th>\n",
       "      <th>embeddings_292</th>\n",
       "      <th>embeddings_293</th>\n",
       "      <th>embeddings_294</th>\n",
       "      <th>embeddings_295</th>\n",
       "      <th>embeddings_296</th>\n",
       "      <th>embeddings_297</th>\n",
       "      <th>embeddings_298</th>\n",
       "      <th>embeddings_299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <th>t5_22i0</th>\n",
       "      <td>-0.068193</td>\n",
       "      <td>-0.140246</td>\n",
       "      <td>-0.078632</td>\n",
       "      <td>-0.088902</td>\n",
       "      <td>0.058947</td>\n",
       "      <td>0.045620</td>\n",
       "      <td>0.037167</td>\n",
       "      <td>-0.058531</td>\n",
       "      <td>-0.037315</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>-0.072647</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>0.147903</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>-0.056435</td>\n",
       "      <td>-0.011115</td>\n",
       "      <td>-0.079805</td>\n",
       "      <td>-0.013302</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>-0.066347</td>\n",
       "      <td>-0.018027</td>\n",
       "      <td>-0.039436</td>\n",
       "      <td>-0.015084</td>\n",
       "      <td>-0.025545</td>\n",
       "      <td>-0.039431</td>\n",
       "      <td>0.049099</td>\n",
       "      <td>-0.045878</td>\n",
       "      <td>-0.074431</td>\n",
       "      <td>-0.021248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>0.020865</td>\n",
       "      <td>0.057544</td>\n",
       "      <td>-0.034896</td>\n",
       "      <td>0.029612</td>\n",
       "      <td>0.048836</td>\n",
       "      <td>0.478561</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>0.054213</td>\n",
       "      <td>-0.006427</td>\n",
       "      <td>0.082874</td>\n",
       "      <td>-0.047048</td>\n",
       "      <td>0.031864</td>\n",
       "      <td>0.053346</td>\n",
       "      <td>-0.027506</td>\n",
       "      <td>-0.012726</td>\n",
       "      <td>0.027782</td>\n",
       "      <td>0.058283</td>\n",
       "      <td>-0.093523</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.035452</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>0.044162</td>\n",
       "      <td>0.014779</td>\n",
       "      <td>-0.027070</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>-0.028784</td>\n",
       "      <td>-0.026580</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de_iama</th>\n",
       "      <th>t5_30305</th>\n",
       "      <td>-0.067955</td>\n",
       "      <td>-0.141612</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.029350</td>\n",
       "      <td>0.088498</td>\n",
       "      <td>0.048055</td>\n",
       "      <td>-0.034850</td>\n",
       "      <td>-0.012829</td>\n",
       "      <td>-0.076761</td>\n",
       "      <td>-0.057956</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.104513</td>\n",
       "      <td>-0.068526</td>\n",
       "      <td>-0.042201</td>\n",
       "      <td>-0.009282</td>\n",
       "      <td>-0.126241</td>\n",
       "      <td>-0.049793</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>-0.107231</td>\n",
       "      <td>-0.038972</td>\n",
       "      <td>-0.005304</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>-0.087264</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.045390</td>\n",
       "      <td>-0.025538</td>\n",
       "      <td>-0.025474</td>\n",
       "      <td>-0.056472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032816</td>\n",
       "      <td>0.080769</td>\n",
       "      <td>0.067391</td>\n",
       "      <td>-0.029824</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>-0.005821</td>\n",
       "      <td>0.320294</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.044636</td>\n",
       "      <td>-0.038056</td>\n",
       "      <td>0.134913</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.050810</td>\n",
       "      <td>0.020896</td>\n",
       "      <td>-0.031264</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>-0.012944</td>\n",
       "      <td>0.083818</td>\n",
       "      <td>-0.006974</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.071032</td>\n",
       "      <td>0.054481</td>\n",
       "      <td>0.017852</td>\n",
       "      <td>-0.003119</td>\n",
       "      <td>-0.028639</td>\n",
       "      <td>0.056255</td>\n",
       "      <td>-0.050546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundesliga</th>\n",
       "      <th>t5_2s82y</th>\n",
       "      <td>-0.091431</td>\n",
       "      <td>-0.087854</td>\n",
       "      <td>-0.145448</td>\n",
       "      <td>-0.009487</td>\n",
       "      <td>0.088647</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>-0.018871</td>\n",
       "      <td>-0.018094</td>\n",
       "      <td>-0.122689</td>\n",
       "      <td>0.063472</td>\n",
       "      <td>0.098198</td>\n",
       "      <td>-0.110660</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.087363</td>\n",
       "      <td>-0.044413</td>\n",
       "      <td>0.021362</td>\n",
       "      <td>0.020865</td>\n",
       "      <td>-0.081610</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>0.201054</td>\n",
       "      <td>-0.144961</td>\n",
       "      <td>0.050591</td>\n",
       "      <td>0.090852</td>\n",
       "      <td>0.070311</td>\n",
       "      <td>0.031639</td>\n",
       "      <td>-0.036719</td>\n",
       "      <td>0.233683</td>\n",
       "      <td>-0.061653</td>\n",
       "      <td>-0.068082</td>\n",
       "      <td>0.132007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219609</td>\n",
       "      <td>-0.055264</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>-0.159257</td>\n",
       "      <td>-0.049292</td>\n",
       "      <td>0.629636</td>\n",
       "      <td>-0.066586</td>\n",
       "      <td>0.114068</td>\n",
       "      <td>0.112220</td>\n",
       "      <td>0.095717</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>-0.155897</td>\n",
       "      <td>0.113290</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>-0.064142</td>\n",
       "      <td>-0.122605</td>\n",
       "      <td>0.053964</td>\n",
       "      <td>-0.065005</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>-0.011948</td>\n",
       "      <td>0.070610</td>\n",
       "      <td>0.049743</td>\n",
       "      <td>-0.055776</td>\n",
       "      <td>-0.090864</td>\n",
       "      <td>-0.042501</td>\n",
       "      <td>-0.092974</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>-0.010848</td>\n",
       "      <td>0.084419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ich_iel</th>\n",
       "      <th>t5_37k29</th>\n",
       "      <td>-0.035952</td>\n",
       "      <td>-0.060456</td>\n",
       "      <td>-0.148823</td>\n",
       "      <td>-0.051870</td>\n",
       "      <td>0.035436</td>\n",
       "      <td>0.035236</td>\n",
       "      <td>-0.006718</td>\n",
       "      <td>-0.063142</td>\n",
       "      <td>-0.043526</td>\n",
       "      <td>-0.111950</td>\n",
       "      <td>-0.023704</td>\n",
       "      <td>-0.049138</td>\n",
       "      <td>0.117564</td>\n",
       "      <td>-0.115526</td>\n",
       "      <td>-0.048981</td>\n",
       "      <td>-0.040843</td>\n",
       "      <td>-0.048785</td>\n",
       "      <td>-0.018597</td>\n",
       "      <td>-0.051052</td>\n",
       "      <td>-0.115204</td>\n",
       "      <td>-0.067233</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>-0.048875</td>\n",
       "      <td>0.021613</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.056340</td>\n",
       "      <td>-0.008354</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056989</td>\n",
       "      <td>-0.010975</td>\n",
       "      <td>0.032579</td>\n",
       "      <td>0.040398</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.025171</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>-0.028034</td>\n",
       "      <td>-0.075452</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.038868</td>\n",
       "      <td>0.088912</td>\n",
       "      <td>0.032729</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.032898</td>\n",
       "      <td>0.060034</td>\n",
       "      <td>-0.031061</td>\n",
       "      <td>-0.084744</td>\n",
       "      <td>-0.080818</td>\n",
       "      <td>0.059550</td>\n",
       "      <td>-0.071320</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>-0.034162</td>\n",
       "      <td>-0.041637</td>\n",
       "      <td>-0.017311</td>\n",
       "      <td>0.018234</td>\n",
       "      <td>-0.026489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <th>t5_2qi4z</th>\n",
       "      <td>-0.109373</td>\n",
       "      <td>-0.060183</td>\n",
       "      <td>-0.221237</td>\n",
       "      <td>-0.129652</td>\n",
       "      <td>0.066976</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>-0.052196</td>\n",
       "      <td>-0.084627</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>0.010767</td>\n",
       "      <td>-0.028676</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.133300</td>\n",
       "      <td>0.054518</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>0.139377</td>\n",
       "      <td>-0.112454</td>\n",
       "      <td>0.098173</td>\n",
       "      <td>0.053897</td>\n",
       "      <td>-0.035679</td>\n",
       "      <td>0.026762</td>\n",
       "      <td>-0.088102</td>\n",
       "      <td>0.107053</td>\n",
       "      <td>0.011507</td>\n",
       "      <td>-0.141544</td>\n",
       "      <td>-0.006828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019560</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.075463</td>\n",
       "      <td>-0.033070</td>\n",
       "      <td>-0.087544</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>0.629335</td>\n",
       "      <td>-0.124546</td>\n",
       "      <td>0.097818</td>\n",
       "      <td>0.233379</td>\n",
       "      <td>0.140266</td>\n",
       "      <td>-0.052758</td>\n",
       "      <td>-0.100207</td>\n",
       "      <td>0.072970</td>\n",
       "      <td>-0.055256</td>\n",
       "      <td>-0.072690</td>\n",
       "      <td>-0.014565</td>\n",
       "      <td>0.071595</td>\n",
       "      <td>-0.043883</td>\n",
       "      <td>0.119549</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>0.033778</td>\n",
       "      <td>0.100645</td>\n",
       "      <td>0.064880</td>\n",
       "      <td>-0.232661</td>\n",
       "      <td>-0.028845</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.036558</td>\n",
       "      <td>-0.118363</td>\n",
       "      <td>-0.018238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9  embeddings_10  embeddings_11  embeddings_12  embeddings_13  embeddings_14  embeddings_15  embeddings_16  embeddings_17  \\\n",
       "subreddit_name subreddit_id                                                                                                                                                                                                                                                                       \n",
       "de             t5_22i0          -0.068193     -0.140246     -0.078632     -0.088902      0.058947      0.045620      0.037167     -0.058531     -0.037315     -0.015350      -0.072647       0.005629       0.147903       0.005428      -0.056435      -0.011115      -0.079805      -0.013302   \n",
       "de_iama        t5_30305         -0.067955     -0.141612     -0.001929     -0.071472     -0.029350      0.088498      0.048055     -0.034850     -0.012829     -0.076761      -0.057956       0.016863       0.104513      -0.068526      -0.042201      -0.009282      -0.126241      -0.049793   \n",
       "bundesliga     t5_2s82y         -0.091431     -0.087854     -0.145448     -0.009487      0.088647      0.062016     -0.018871     -0.018094     -0.122689      0.063472       0.098198      -0.110660       0.090146       0.087363      -0.044413       0.021362       0.020865      -0.081610   \n",
       "ich_iel        t5_37k29         -0.035952     -0.060456     -0.148823     -0.051870      0.035436      0.035236     -0.006718     -0.063142     -0.043526     -0.111950      -0.023704      -0.049138       0.117564      -0.115526      -0.048981      -0.040843      -0.048785      -0.018597   \n",
       "germany        t5_2qi4z         -0.109373     -0.060183     -0.221237     -0.129652      0.066976      0.051033      0.076763     -0.052196     -0.084627      0.099022       0.009620       0.010689       0.010767      -0.028676       0.002753      -0.000592      -0.133300       0.054518   \n",
       "\n",
       "                             embeddings_18  embeddings_19  embeddings_20  embeddings_21  embeddings_22  embeddings_23  embeddings_24  embeddings_25  embeddings_26  embeddings_27  embeddings_28  embeddings_29  ...  embeddings_270  embeddings_271  embeddings_272  embeddings_273  embeddings_274  \\\n",
       "subreddit_name subreddit_id                                                                                                                                                                                      ...                                                                                   \n",
       "de             t5_22i0            0.024932       0.001717      -0.066347      -0.018027      -0.039436      -0.015084      -0.025545      -0.039431       0.049099      -0.045878      -0.074431      -0.021248  ...        0.015191        0.020865        0.057544       -0.034896        0.029612   \n",
       "de_iama        t5_30305           0.015492      -0.107231      -0.038972      -0.005304      -0.007023       0.007038      -0.087264       0.000271      -0.045390      -0.025538      -0.025474      -0.056472  ...        0.032816        0.080769        0.067391       -0.029824        0.046361   \n",
       "bundesliga     t5_2s82y           0.098145       0.201054      -0.144961       0.050591       0.090852       0.070311       0.031639      -0.036719       0.233683      -0.061653      -0.068082       0.132007  ...       -0.219609       -0.055264        0.033242        0.015594       -0.159257   \n",
       "ich_iel        t5_37k29          -0.051052      -0.115204      -0.067233      -0.017151       0.089802      -0.048875       0.021613       0.010931       0.023022       0.056340      -0.008354       0.004250  ...        0.056989       -0.010975        0.032579        0.040398        0.000713   \n",
       "germany        t5_2qi4z          -0.106768       0.139377      -0.112454       0.098173       0.053897      -0.035679       0.026762      -0.088102       0.107053       0.011507      -0.141544      -0.006828  ...       -0.019560        0.026004        0.075463       -0.033070       -0.087544   \n",
       "\n",
       "                             embeddings_275  embeddings_276  embeddings_277  embeddings_278  embeddings_279  embeddings_280  embeddings_281  embeddings_282  embeddings_283  embeddings_284  embeddings_285  embeddings_286  embeddings_287  embeddings_288  embeddings_289  embeddings_290  \\\n",
       "subreddit_name subreddit_id                                                                                                                                                                                                                                                                   \n",
       "de             t5_22i0             0.048836        0.478561       -0.029383        0.054213       -0.006427        0.082874       -0.047048        0.031864        0.053346       -0.027506       -0.012726        0.027782        0.058283       -0.093523        0.033228        0.035452   \n",
       "de_iama        t5_30305           -0.005821        0.320294        0.033977        0.044636       -0.038056        0.134913        0.009774        0.050810        0.020896       -0.031264        0.027764       -0.012944        0.083818       -0.006974        0.120271        0.020580   \n",
       "bundesliga     t5_2s82y           -0.049292        0.629636       -0.066586        0.114068        0.112220        0.095717        0.020551       -0.155897        0.113290        0.044877       -0.064142       -0.122605        0.053964       -0.065005        0.000735       -0.011948   \n",
       "ich_iel        t5_37k29            0.025171        0.477503        0.028220       -0.028034       -0.075452        0.003443        0.038868        0.088912        0.032729        0.007014        0.032898        0.060034       -0.031061       -0.084744       -0.080818        0.059550   \n",
       "germany        t5_2qi4z            0.025086        0.629335       -0.124546        0.097818        0.233379        0.140266       -0.052758       -0.100207        0.072970       -0.055256       -0.072690       -0.014565        0.071595       -0.043883        0.119549        0.017734   \n",
       "\n",
       "                             embeddings_291  embeddings_292  embeddings_293  embeddings_294  embeddings_295  embeddings_296  embeddings_297  embeddings_298  embeddings_299  \n",
       "subreddit_name subreddit_id                                                                                                                                                  \n",
       "de             t5_22i0             0.045030        0.044162        0.014779       -0.027070        0.004733       -0.002979       -0.028784       -0.026580        0.000027  \n",
       "de_iama        t5_30305            0.002752        0.028324        0.071032        0.054481        0.017852       -0.003119       -0.028639        0.056255       -0.050546  \n",
       "bundesliga     t5_2s82y            0.070610        0.049743       -0.055776       -0.090864       -0.042501       -0.092974       -0.002848       -0.010848        0.084419  \n",
       "ich_iel        t5_37k29           -0.071320        0.013347        0.012645        0.037299       -0.034162       -0.041637       -0.017311        0.018234       -0.026489  \n",
       "germany        t5_2qi4z            0.033778        0.100645        0.064880       -0.232661       -0.028845       -0.078855       -0.036558       -0.118363       -0.018238  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc87b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
